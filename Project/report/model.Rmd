# Model 
 
```{r confusion matrix draw function, include=FALSE}

draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, '0', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, '1', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, '0', cex=1.2, srt=90)
  text(140, 335, '1', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", 
       main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

```

## Selecting modelling technique 

The modelling technique that we will be using are the following:


| n°  | Model                      | Function in R |
|:---:|:---------------------------|:--------------|
|  1  | Logistic regression models |    GLM        |
|  2  | Decision trees             |   CART        |
|  3  | Discriminate analysis      | 1) lda <br /> 2) qda <br /> 3) mda <br /> 4) fda |
|  4  | Random forest              |  randomForest |
|  5  | XGBoost                    |    xgboost    |

## Generate test design

Firstly, we need to standardize the data, as the variables have different scales. We will normalize only the continuous variables, as the categorical and dummy variables have only few different levels.  

```{r scaling}

#selecting only the continuous variables to scale them

data_scale <- data_sel %>% 
                dplyr::select(DURATION, AMOUNT, INSTALL_RATE, AGE, NUM_CREDITS) %>% 
                scale() %>%  #normalization
                as.data.frame()

#recreating the other variables to add them back to the dataset of the scaled ones 

data_scale %<>% mutate(
  CHK_ACCT = data_sel$CHK_ACCT,
  HISTORY = data_sel$HISTORY,
  PURPOSE = data_sel$PURPOSE,
  SAV_ACCT = data_sel$SAV_ACCT,
  EMPLOYMENT = data_sel$EMPLOYMENT,
  SEX_MALE = data_sel$SEX_MALE,
  MALE_SINGLE = data_sel$MALE_SINGLE,
  MALE_MAR_WID = data_sel$MALE_MAR_WID,
  CO_APPLICANT = data_sel$CO_APPLICANT,
  GUARANTOR = data_sel$GUARANTOR,
  PRESENT_RESIDENT = data_sel$PRESENT_RESIDENT,
  PROPERTY = data_sel$PROPERTY,
  OTHER_INSTALL = data_sel$OTHER_INSTALL,
  RESIDENCE = data_sel$RESIDENCE,
  JOB = data_sel$JOB, 
  RESPONSE = data_sel$RESPONSE
)

#reordering variable in the dataset

data_scale %<>% 
  dplyr::select(CHK_ACCT,DURATION,HISTORY,PURPOSE,AMOUNT,SAV_ACCT,EMPLOYMENT,
         INSTALL_RATE,SEX_MALE,MALE_SINGLE,MALE_MAR_WID,CO_APPLICANT,GUARANTOR,
         PRESENT_RESIDENT,PROPERTY,AGE,OTHER_INSTALL,RESIDENCE,NUM_CREDITS,JOB,RESPONSE)

```

We will then move on by creating a training and test set based on the data.

This will be done by dividing it in a randomly selection into the two subsets, with 75% of the data in the training set and the remaining 25% in the test set.

```{r test and training sets}

#so that we always have the same division
set.seed(2311)

#creation of the index to divide the data in the two subsets
val_index<-createDataPartition(data_scale$RESPONSE, 
                               p=0.75, list=FALSE)

#training dataset
TrainData<-as.data.frame(data_scale[val_index,])

#test dataset
TestData <- data_scale[-val_index,]

```

We can now move to the modelization. Let's start with the first model. 

## Build models 

### 1) Logistic Regressions


\[ Z_{i} = ln(\frac{P_{i}} {1-P_{i}}) = \beta_0+\beta_1X_1+...+\beta_nX_n \]

\[ Z{i} = ln(\sum\limits_{i,j} \frac{(Observed-Expected)^2} {Expected}), \]


#### GLM

##### Definition

We will start by giving a definition of this model. 

> Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). 
(https://en.wikipedia.org/wiki/Logistic_regression)

To do so, we are going follow 6 steps:

2) Check for class bias
3) Create training and test samples
4) Compute information value to find out important variables
5) Build logit models and predict on test data
6) Do model diagnostics

##### Fitting the model 

Now we will fit the logistic regression on the training dataset.

```{r glm-model, echo=FALSE, warning=FALSE, message=FALSE}


table(data_sel$RESPONSE) #--> there is a class bias in the data in general
table(TrainData$RESPONSE) #--> there is a class bias in the training


#Same division
set.seed(1234)

#model
glm.fit <- glm(RESPONSE ~., data = TrainData, family = binomial(link="logit")) #family=binomial is key for classification

#check outputs
summary(glm.fit)

#check which variable is significance--> high coefficient = bring high information
temp <- summary(glm.fit)$coeff[-1,4] < 0.05

#condition: yes or not
temp %<>% as.data.frame()  

kable(temp, caption = "Significance of variable")

#remove variable temp
rm(temp) 

```

Here, we can see that the variables that take the highest importance and that are statistically significant for the model are: the second and third level of CHK_ACC, DURATION, the fourth level of HISTORY,the first level of PURPOSE, AMOUNT,the fourth level of SAV_ACCT, the second, third and fourth level of EMPLOYMENT, MALE_SINGLE (being positive), and OTHER_INSTALL (being positive).

```{r glm coef}

glm.fit$coefficients

```

If we look at the coeffiecient of the different variables we can conclude that CHK_ACCT, HISTORY, SAV_ACCT, EMPLOYMENT and MALE_SINGLE, have a positive impact on the output, meaning that the higher is their level or if they are positive, the probability of having RESPONSE = 1 will increase. 

On the other hand, DURATION, PURPOSE, AMOUNT and OTHER_INSTALL have a negative effect on the output, meaning that if they increase their level or value, or if they have a positive value (for the dummies), the probability of having a positive response will decrease.

##### Predictions and confusion matrix 

Now we will get the predictions using this model. To do it, we will start by getting the probabilities of the output given the coefficients we have found by fitting the model, then we will use a cut point of 0.5 to decide whether the value will be equal to 1 (if the probability it higher than 0.5) or o (otherwise).

```{r predictions}

#probability given the model
glm.probs <- predict(glm.fit, newdata = TestData, type = "response")  #predict give me the probability i am looking for the the binomial answer

#use a cut of 0.5 to give the prediction
glm.pred <- ifelse(glm.probs > 0.5, 1, 0)


```

This are the predictions for the RESPONSE variable, given the glm model. We use them and a thershold of 0.5 to determine the prediction, whether the person we examine will get a credit (RESPONE = 1), or not (RESPONSE = 0).

##### Model diagnosis 



```{r predictions}

#first lest check the variables that are not significance: Multiculinearity verification

car::vif(glm.fit)


```



##### Predictions and confusion matrix 

Then, with the prediction we can build the confusion matrix. 


```{r confusion matrix}

#confusion matrix 
cm <- confusionMatrix(as.factor(glm.pred), as.factor(TestData$RESPONSE))
#draw of confusion matrix
draw_confusion_matrix(cm)
#remove temporary element
rm(cm)

```

We are more interested in the false prediction of negative outcomes, as it would mean a loss for the company, rather than a false prediction of a positive outcome, hence we will look at the specificity rather than the sensitivity. Here we can see the the prediction of negative values is quite high (above 90% of the time when an element is predicted as negative it if truly negative) and also if it is negative it will be predicted as such. 

We could beblieve that this is due to an unbalance of the data set, but as we have seen in the EDA, we have a dataset with a higher presence of positive value than negative, hence it would be easier for the model to predict a positive value, which is not the case. 

We will repeat the prediciton with a selection of the model based in the AIC using the step function of R.

> The Akaike information criterion (AIC) is a mathematical method for evaluating how well a model fits the data it was generated from. In statistics, AIC is used to compare different possible models and determine which one is the best fit for the data.
source: https://www.scribbr.com/statistics/akaike-information-criterion/

The step function follows the idea that the variable that increases the AIC of the model the most will be discarder, up to the point in which it is not possible to decrease the AIC anymore. 

```{r glm selection}

#selections based on the AIC of the model
glm.sel <- step(glm.fit)
#summary of selected model
summary(glm.sel)
#probability given the model
glm.sel.probs <- predict(glm.sel, newdata = TestData, type = "response")
#use a cut of 0.5 to give the prediction
glm.sel.pred <- ifelse(glm.sel.probs > 0.5, 1, 0)
#confusion matrix 
cm <- confusionMatrix(as.factor(glm.sel.pred), as.factor(TestData$RESPONSE))
#draw of confusion matrix
draw_confusion_matrix(cm)
#remove temporary element
rm(cm)

```
We can see that this model is suprisingly performing worse than the previous one, as sensitivy, specificity and accuracy have all a lower value with respect to the one of the non-selected model.  

### Decision Trees

#### CART

##### Definition

> A Classification And Regression Tree (CART), is a predictive model, which explains how an outcome variable's values can be predicted based on other values. A CART output is a decision tree where each fork is a split in a predictor variable and each end node contains a prediction for the outcome variable.

(source: https://wiki.q-researchsoftware.com/wiki/Machine_Learning_-_Classification_And_Regression_Trees_(CART))

##### Fitting the model 

We will start by fitting the model on the data.

```{r cart}

#library(rpart)
#library(rpart.plot)
ct.fit <- rpart(RESPONSE ~., method = "class",  
                data = TrainData, #the training data
                control = rpart.control(minsplit = 4, 
                cp = 1e-05) , model = TRUE)

summary(ct.fit)

```

We will then prune the tree (meaning that we will try to discard the variables that are not important for the classificaiton of the output) by looking for the best complexity parameter. 

> Pruning is a Data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are uncritical and redundant to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.
source:https://en.wikipedia.org/wiki/Decision_tree_pruning


```{r pruning cart, echo=FALSE}

#cp graph
par(pty = "s")
plotcp(ct.fit)

#cp that minimize the tree
bestcp <- ct.fit$cptable[which.min(ct.fit$cptable[,"xerror"]),"CP"]

ct.prune <- prune(ct.fit, cp= bestcp) #before there was cp=0.01

summary(ct.prune)

#first option of graph:
#plot(ct.prune, uniform = TRUE)
#text(ct.prune, cex = 0.4, use.n = TRUE, all = TRUE)

#second option of graph:
rpart.plot(ct.prune, extra=104, box.palette="GnBu",
               branch.lty=3, shadow.col="gray", nn=TRUE)


#partition tree--> lets check later....
set.seed(123) ## For consistent jitter

partition_data <- TrainData
partition_data$RESPONSE <- as.factor(partition_data$RESPONSE)

## Build our tree using parsnip (but with rpart as the model engine)

t_tree <-
  decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  fit(RESPONSE ~ CHK_ACCT + HISTORY , data = partition_data)

#install.packages("remotes")
# remotes::install_github("grantmcdermott/parttree") 
# library(parttree) not working

## Plot the data and model partitions
# partition_data %>%
#   ggplot(aes(x=CHK_ACCT, y=HISTORY)) + #axis to check
#   geom_jitter(aes(col=RESPONSE), alpha=0.7) +
#   geom_parttree(data = t_tree, aes(fill=RESPONSE), alpha = 0.1) +
#   theme_minimal()+
#   theme_bw()


```

The pruned tree has a number of split equal to 5 and it consider the variables CHK_ACCT, HISTORY, PURPOSE, AMOUNT, NUM_CREDITS, PRESENT_RESIDENT. 

The graph that we have creted shows us how a new observation would be classified:
1. If the CHK_ACCT is  equal to 0 or 1 go to the next node, otherwise the RESPONSE will be 1;
2. It the HISTORY is equal to 0, 1 or 2 go the next node, otherwise REPSONSE will be 1;
3. If PURPOSE is equal to 1 or 5 the RESPONSE will be 0, otherwise go the next node;
4. If AMOUNT is lower than 1.7, the RESPONSE will be 0, otherwise go the next node;
5. If DURATION is higher than 1.9 RESPONSE will 0, otherwise RESPONSE will be 1. 

##### Predictions and confusion matrix 

We will now use the two models (the basic one and the pruned one) to predict the values for the output variable and then build the confusion matrix. 
The process will be the one described above for each observation that is presenent in the test set. 

```{r prediction cart}

ct.pred <- predict(ct.fit, newdata = TestData, type = "class")
ct.prune.pred <- predict(ct.prune, newdata = TestData, type = "class")

```


```{r confusion matric cart}

#confusion matrix of the basic model
cm <- confusionMatrix(ct.pred, TestData$RESPONSE)
#drawing cm for basic model
draw_confusion_matrix(cm)
#removing temporary element
rm(cm)

#confusion matrix of pruned model
cm <- confusionMatrix(as.factor(ct.prune.pred), as.factor(TestData$RESPONSE))
#draw cm of pruned model
draw_confusion_matrix(cm)
#remove temporary element 
rm(cm)

```

We can see that here the sensitivity is really low in both cases, but is lower for the pruned model, while the specificity is higher, and it increases even more in the pruned model reaching a value above 90%, which is in any case the one in which we are the most interested. Also the accuracy increases in the pruned model. 

### Discriminate analysis 

#### LDA 

##### Definition

We look now at the LDA, which is defined as:

> The Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.
(source: http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/)

##### Fitting the model

We start by fitting the model

```{r lda model}

#we need to create a reference for each observation to use this model 
OBS <- 1:750

#model creation
lda.fit <- lda(RESPONSE ~., data = TrainData[,-1], subset = OBS)

lda.fit

#A Stacked Histogram of the LDA Values 
#The two groups are the groups for response classes.
plot(lda.fit)

```

Hence, the linear combination of predictor variables that are used to form the decision rule is the following: 
$$ RESPONSE = -0.3265 * DURATION -0.2747 * HISTORY_1 + 0.8792 * HISTORY_2 + 1.1810 * HISTORY_3 + 1.6214 * HISTORY_4 - 0.7437 * PURPOSE_1 + 0.7736 * PURPOSE_2 + 0.0172 * PURPOSE_3 + 0.2035 * PURPOSE_4 - 0.6116 * PURPOSE_5 + 0.1298 * PURPOSE_6 - 0.2579 * AMOUNT + 0.5066 * SAV_ACCT_1 + 0.7517 * SAV_ACCT_2 + 0.7778 * SAV_ACCT_3 + 1.0997 * SAV_ACCT_4 + 0.6175 * EMPLOYMENT_1 + 1.1982 * EMPLOYMENT_2 + 1.4580 * EMPLOYMENT_3 + 1.1806 * EMPLOYMENT_4 - 0.2897 * INSTALL_RATE - 0.5663 * SEX_MALE_1 + 0.9272 * MALE_SINGLE_1 + 0.5183 * MALE_MAR_WID_1 - 0.0863 * CO_APPLICANT_1 + 0.5084 * GUARANTOR_1 - 0.2437 * PRESENT_RESIDENT_-1 - 0.1913 * PRESENT_RESIDENT_0 - 0.0477 * PRESENT_RESIDENT_1 + 0.0367 * PROPERTY_1 - 0.6374 * PROPERTY_2 + 0.0502 * AGE - 0.4501 * OTHER_INSTALL_1 - 0.7509 * RESIDENCE_1 - 0.2185 * RESIDENCE_2 - 0.0703 * NUM_CREDITS - 1.0570 * JOB_1 - 1.0581 * JOB_2 - 0.8362 * JOB_3$$

Each new observation will be evaluated thanks to this formula, with its information put inside of it. To be clear, if for example the purpose variable takes value 3, only the coefficient of $$PURPOSE_3$$ will be added to the others. The same goes for each other categorical variable. For the dummies the coefficient is added only if the value is equal to 1 for the variable, otherwise no. While for the continuous variables the coefficient is multiplied by the value is recorded in the observation. 

##### Predictions and confusion matrix 

Using the formula above we can get the predictions and subsequently construct the confusion matrix.

```{r lda predictions}

lda.pred <- predict(lda.fit, newdata = TestData)
lda.class <- lda.pred$class


#plot the error of the predictions

par(mfrow=c(1,1))
plot(lda.pred$x[,1], lda.pred$class, col=TestData$RESPONSE)
#key takeaway
#The Predicted Group-1 and Group-2 has been colored (red and green). The mix of red and green color in each group shows the incorrect classification prediction.

```
In the graph, the red dots corresponds to the incorrect classification preditions. We can see that the majority are in the positive values, which is quite bad for the company, as it would mean granting a credit to someone who is actually at high risk. 

```{r confusion matrix lda}

#building the confusion matrix for the lda 
cm <- confusionMatrix(as.factor(lda.class), as.factor(TestData$RESPONSE))
#drawinf the cf 
draw_confusion_matrix(cm)
#removing the temporary element
rm(cm)

```
 
Here the sensitivity is higher with respect to the previous models (above 40%), but it is still quite low. 
 
#### QDA 

##### Definition

We now look at the Quadratic Discriminant Analysis, which is defined in the following way:

> Quadratic Discriminant Analysis is little bit more flexible than LDA, in the sense that it does not assumes the equality of variance/covariance. In other words, for QDA the covariance matrix can be different for each class.

(source: http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/)

##### Fitting the model 

```{r fit qda}

#fitting the qda model
qda.fit <- qda(RESPONSE ~., data = TrainData, subset = OBS)
#get the result
qda.fit

```

However, as our sample is quite small, we predict that the LDA will be a better fit compared to the QDA. 

##### Predictions and confusion matrix 

```{r qda predictions}

#getting the overall predictions
qda.pred <- predict(qda.fit, newdata = TestData)
#extracting only the prediciton values
qda.class <- qda.pred$class

#plotting predictions against real data
par(mfrow=c(1,1))
plot(qda.pred$posterior[,2], qda.pred$class, col=TestData$RESPONSE)

```

```{r confusion matrix qda}

#building the confusion matrix for the QDA
cm <- confusionMatrix(as.factor(qda.class), as.factor(TestData$RESPONSE))
#drwaing the matrix
draw_confusion_matrix(cm)
#removing the temporary element
rm(cm)

``` 

Here the sensitivity is the highest up to now (over 50%), is still quite low though. The specificity is moderately high (above 85%) as it is the accuracy (above 70%). 

<!-- More on interpretation when we are sure we need to look at specificity  -->
 
#### MDA 

##### Definition

The third model of discriminant analysis is the MDA:

> For Mixture Discriminant Analysis, there are classes, and each class is assumed to be a Gaussian mixture of subclasses, where each data point has a probability of belonging to each class. Equality of covariance matrix, among classes, is still assumed.

(source: http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/)

##### Fitting the model 

We can fit the model and look at what it gives as result.

```{r fit mda}

#model fitting
mda.fit <- mda(RESPONSE ~., data = TrainData, subset = OBS)
#model summary
(mda.fit)

```

The summary gives the percentage of the variance that there is inside the different groups that has been created (which in this case are 5), and we can see that the vast majority of the variance is explained thanks to the first three groups (reaching more than 90%), but we could also be satisfied only considering the first two groups (as they each almost 80% of the variance explained).

##### Predictions and confusion matrix 

Using the model we get the prediction for the RESPONSE variable and we can construct the confidence matrix for this case.

```{r mda predictions}

mda.pred <- predict(mda.fit, newdata = TestData)

```

```{r confusion matrix mda}

#confusion matrix builiding
cm <- confusionMatrix(as.factor(mda.pred), as.factor(TestData$RESPONSE))
#cm draw
draw_confusion_matrix(cm)
#remove temporary element
rm(cm)

```  

The sensitivity in this case is around 45%, while the specificity is higher, reaching almost 88%. The accurary is around 75%. 

#### FDA 

##### Definition

The last model of discriminant analysis is the FDA, which is by definition:

> Flexible Discriminant Analysis is a flexible extension of LDA that uses non-linear combinations of predictors such as splines. FDA is useful to model multivariate non-normality or non-linear relationships among variables within each group, allowing for a more accurate classification.

(source: http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/)

##### Fitting the model 

We can now fit the model and get the summary of the results.

```{r fit qda}

fda.fit <- fda(RESPONSE ~., data = TrainData, subset = OBS)
fda.fit

```

Here we see that this model has only one dimensino that, obviously, explains the 100% of the between group variance. We expect that it will perfom poorly in the predicitons.

##### Predictions and confusion matrix 

Using the model we calculate the predicitons for the output variable and then the confusion matrix based on them.

```{r lda predictions}

fda.pred <- predict(fda.fit, newdata = TestData)

```

```{r confusion matrix lda}

#bulding cm
cm <- confusionMatrix(as.factor(fda.pred), as.factor(TestData$RESPONSE))
#drawing cm
draw_confusion_matrix(cm)
#remove cm element
rm(cm)

```  

Contrary to what it is expected, this model has a sensitivity of almost 50%, among one of the highest up to now, and the specificity is higher than 90%. The accuracy is around 78%. 

### Ensemble Methods

#### Random Forest

##### Definition

We now move one and we look at the random forest model, which is defined in the following way:

> Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees. 

(source: https://en.wikipedia.org/wiki/Random_forest)

##### Fitting the model

We can start by fitting the model and look at the results it gives.

```{r random forest fit}

#fitting random forest
rf.fit <- randomForest(RESPONSE ~., 
                       TrainData, 
                       ntree = 500, 
                       mtry = 4, 
                       importance = TRUE, 
                       replace = FALSE)

```


```{r results rf, fig.asp=1.5}

print(rf.fit)

#Variable importance analysis

importance(rf.fit) 
varImpPlot(rf.fit, main ='xxxx')

#two criteria: MeanDecreaseAccuracy (rough estimate of the loss in prediction performance when that particular variable is omitted from the training set) and Mean Decrease Gini (GINI is a measure of node impurity, highest purity means that each node contains only elements of a single class. Assessing the decrease in GINI when that feature is omitted leads to an understanding of how important that feature is to split the data correctly)

```

The summary of the models gives the decrease in accuracy and the decrease of the gini index for each variable in the model, along with the number of trees that are built (500 in our case), the number of variabes that are randomly chones to be tried at each split before choosing which one is the best one to describe the node. Moreover, we can already find the confusion matrix (we will show it better again afterwards to keep the coherence of the analysis throuhgout all the models), with the class errorand the Out-Of-Bag estimate of the error rate. 

Let's give some definitions to be clearer:

> Variable importance is the mean decrease of accuracy over all out-of-bag cross validated predictions, when a given variable is permuted after training, but before prediction.

> GINI importance measures the average gain of purity by splits of a given variable. If the variable is useful, it tends to split mixed labeled nodes into pure single class nodes. Splitting by a permuted variables tend neither to increase nor decrease node purities. 

source:https://stats.stackexchange.com/questions/197827/how-to-interpret-mean-decrease-in-accuracy-and-mean-decrease-gini-in-random-fore

> Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging) to sub-sample data samples used for training. OOB is the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample.
source: https://en.wikipedia.org/wiki/Out-of-bag_error

##### Predictions and confusion matrix 

We can now use the model to get the predictions for the output variable and build the confusion matrix.

```{r rf prediction}

#obtain probability
rf.pred <- predict(rf.fit, newdata = TestData, type = "class")

```


```{r rf confusion matrix}

#build cm for rf 
cm <- confusionMatrix(as.factor(rf.pred), as.factor(TestData$RESPONSE))
#draw the cm
draw_confusion_matrix(cm)
#remove cm element
rm(cm)

```

Here the sensitivity is around 46%, the specificity is high (more than 90%) and the accuracy is aroun 77%. 

### XGBoost 

#### Model name

##### Definition

##### Fitting the model 


```{r fit xgboost}

#we change everything to a factor or numerico

data_xgboost <- map_df(data_sel, function(columna) {
                  columna %>% 
                  as.factor() %>% 
                  as.numeric %>% 
                  { . - 1 } })

test_xgboost <- sample_frac(data_xgboost, size = 0.249)
train_xgboost <- setdiff(data_xgboost, test_xgboost)


#Convertir a DMatrix

train_xgb_matrix <-   train_xgboost %>% 
                            dplyr::select(- RESPONSE) %>% 
                            as.matrix() %>% 
                            xgb.DMatrix(data = ., label = train_xgboost$RESPONSE)
#Convertir a DMatrix

test_xgb_matrix <-  test_xgboost %>% 
                            dplyr::select(- RESPONSE) %>% 
                            as.matrix() %>% 
                            xgb.DMatrix(data = ., label = test_xgboost$RESPONSE)

#####################Define the parameters##########################################

#objective = "binary:logistic": we will train a binary classification model ;
#max.depth = 2: the trees won’t be deep ;
#nthread = 2: the number of cpu threads we are going to use;
#nrounds = there will be two passes on the data, the second one will enhance the model by further reducing the difference between ground truth and prediction.

set.seed(124)
model_xgboost <- xgboost::xgboost(data = train_xgb_matrix , 
                                    objective = "binary:logistic",
                                    nrounds = 20, max.depth = 2, eta = 0.3, 
                                    nthread = 2, subsample = 1)


model_xgboost


####################Training & Test Error Plot ####################
#if they are more close better the accuracy

e <- data.frame(model_xgboost$evaluation_log)
plot(e$iter, e$train_mlogloss, col = 'blue')
lines(e$iter, e$test_mlogloss, col = 'red')

##################### Important features###############

imp_feature <- xgb.importance(colnames(train_xgb_matrix), model = model_xgboost)
print(imp_feature)
xgb.plot.importance(imp_feature)

#####################Predictions with the model ##########################################

predict_xgb1 <- predict(model_xgboost, test_xgb_matrix)
#predict_xgb1 <- as.data.frame(predict_xgb1) #---> if we want to create a kagle
xgboost.pred <- ifelse(predict_xgb1 > 0.5, 1, 0) #change the probability to binory output

####################Confusion Matrix#################

#bulding cm
cm <- caret::confusionMatrix(as.factor(xgboost.pred), as.factor(TestData$RESPONSE))
#################draw of confusion matrix
draw_confusion_matrix(cm)
##############vv#remove temporary element
rm(cm)


```



##### Hyper-parameter Tuning

```{r xgboost tuning}

###############################################

model_xgboost_balance <- xgboost::xgboost(data = train_xgb_matrix ,
                                          objective = "binary:logistic",
                                          nrounds = 20, max.depth = 2, eta = 0.3, nthread = 2,
                                          subsample = 0.5 ) ###balance
                                    
model_xgboost_balance

```


##### Predictions 

```{r xgboost predictions}

#####################Predictions with the model ##########################################

predict_xgb1_balance <- predict(model_xgboost_balance, test_xgb_matrix)
#predict_xgb1 <- as.data.frame(predict_xgb1) #---> if we want to create a kagle
xgboost.pred_balance <- ifelse(predict_xgb1_balance > 0.5, 1, 0) #change the probability to binary output

```


```{r confusion matrix xgboost}
####################Confusion Matrix#################

#bulding cm
cm <- caret::confusionMatrix(as.factor(xgboost.pred_balance), as.factor(TestData$RESPONSE))
#################draw of confusion matrix
draw_confusion_matrix(cm)
##############vv#remove temporary element
rm(cm)


```  

### XGBoost 

#### Model name

##### Definition

##### Fitting the model 


```{r fit neural}

#library(neuralnet) 
set.seed(333) 
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5)
nn.fit <- caret::train(RESPONSE ~ ., TrainData, method='nnet', trace = FALSE, 
                       ### Parameters for optmization
                       ###learningrate = 0.0005, threshold = 0.005, stepmax = 1e+07, 
                       trControl= train_params)

predict_nn <- predict(nn.fit, TestData)

#library(nnet)
#library(NeuralNetTools)
plotnet(nn.fit$finalModel, y_names = "yes/no")
title("Graphical Representation of our Neural Network")


####################Confusion Matrix#################
#bulding cm
cm <- caret::confusionMatrix(as.factor(predict_nn), as.factor(TestData$RESPONSE))
#################draw of confusion matrix
draw_confusion_matrix(cm)
##############vv#remove temporary element
rm(cm)


```



##### Hyper-parameter Tuning

```{r neural tuning}

###############################################

set.seed(333) 
train_params_balance <- trainControl(method = "repeatedcv", number = 10, 
                                     repeats=5, sampling = "down" )

nn.fit_balance <- caret::train(RESPONSE ~ ., TrainData, method='nnet', trace = FALSE, 
                       ### Parameters for optmization
                       ###learningrate = 0.0005, threshold = 0.005, stepmax = 1e+07, 
                       trControl= train_params)

```


##### Predictions 

```{r neural predictions}

#####################Predictions with the model ##########################################

predict_nn_balance <- predict(nn.fit_balance, TestData)

#library(nnet)
#library(NeuralNetTools)
plotnet(nn.fit_balance$finalModel, y_names = "yes/no")
title("Graphical Representation of our Neural Network")



```


```{r confusion matrix neural}
####################Confusion Matrix#################

####################Confusion Matrix#################
#bulding cm
cm <- caret::confusionMatrix(as.factor(predict_nn_balance), as.factor(TestData$RESPONSE))
#################draw of confusion matrix
draw_confusion_matrix(cm)
##############vv#remove temporary element
rm(cm)


```  



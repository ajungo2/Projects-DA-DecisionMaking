---
title: "Model"
output:
        bookdown::html_document2: default
      
---
# Model 

```{r, include=FALSE}

############# Function for draw the confussion matrix####################################

draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, '0', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, '1', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, '0', cex=1.2, srt=90)
  text(140, 335, '1', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", 
       main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

```


## Select modeling technique

The modelling technique that we will be using are the following:

| n° | Model        |  Definition                                                       | 
|:--:|:-------------|:------------------------------------------------------------------|   
| 1  | Logistic <br /> regression   | > Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). <br /> (https://en.wikipedia.org/wiki/Logistic_regression) |
| 2  | Decision <br /> trees        | > A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.<br /> (https://en.wikipedia.org/wiki/Decision_tree) |
| 3  | Discriminate <br /> analysis |  > Discriminant analysis is statistical technique used to classify observations into non-overlapping groups, based on scores on one or more quantitative predictor variables.<br /> (https://stattrek.com/multiple-regression/discriminant-analysis.aspx)|
| 4  | Random <br /> forest | > Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees. <br /> (https://en.wikipedia.org/wiki/Random_forest)|
| 5  | Neural <br /> network| > A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. <br /> (https://en.wikipedia.org/wiki/Neural_network)  |
| 6  | XGBoost              | > XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. <br /> (https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)|

In order to compare the 6 models shown above, we will mainly use the *CARET package* for each algorithm. 

## Generate test design 

$H_ {0}$: The $Model_n$ give the best accuracy and sensitivity.

$H_ {1}$: It do not give the best values. 

Where $n= (1,2,3,4,5,6)$ and it represents each listed model in the selection technique part.

## Build model

To be able to generate the model, first, we need to standardize the data, as the variables have different scales. Nevertheless, we will normalize only the continuous variables, as the categorical and dummy variables have only few different levels.  

```{r, warning=FALSE, message=FALSE, echo=FALSE}

#selecting only the continuous variables to scale them

p1<- data_sel %>%  
      tidyr::gather(variable, value, c("DURATION", "AMOUNT",  #we do not include age
                                       "INSTALL_RATE","NUM_CREDITS")) %>% 
      ggplot(aes(x = variable, y = value, fill = variable)) + 
      geom_boxplot() +
      theme_bw() +
      coord_flip()+
      theme(legend.position="none")+
      theme(legend.title = element_blank()) + 
      theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
      labs(title = "Non-standardized variables",  
           x = "Variables", y = "Value (units)")
      
      
data_scale <- data_sel %>% 
                dplyr::select(DURATION, AMOUNT, #we eliminate AGE as well 
                              INSTALL_RATE, NUM_CREDITS) %>% 
                scale() %>%  #normalization
                as.data.frame()

#recreating the other variables to add them back to the dataset of the scaled ones 

data_scale %<>% mutate(
  CHK_ACCT = data_sel$CHK_ACCT,
  HISTORY = data_sel$HISTORY,
  PURPOSE = data_sel$PURPOSE,
  SAV_ACCT = data_sel$SAV_ACCT,
  EMPLOYMENT = data_sel$EMPLOYMENT,
  SEX_MALE = data_sel$SEX_MALE,
  MALE_SINGLE = data_sel$MALE_SINGLE,
  MALE_MAR_WID = data_sel$MALE_MAR_WID,
  CO_APPLICANT = data_sel$CO_APPLICANT,
  GUARANTOR = data_sel$GUARANTOR,
  PRESENT_RESIDENT = data_sel$PRESENT_RESIDENT,
  PROPERTY = data_sel$PROPERTY,
  OTHER_INSTALL = data_sel$OTHER_INSTALL,
  RESIDENCE = data_sel$RESIDENCE,
  JOB = data_sel$JOB, 
  RESPONSE = data_sel$RESPONSE
)

#reordering variable in the dataset- we do not include the not selected variables

data_scale %<>% 
  dplyr::select(CHK_ACCT,DURATION,HISTORY,PURPOSE,AMOUNT,SAV_ACCT,EMPLOYMENT,
         INSTALL_RATE, MALE_SINGLE,GUARANTOR,
         PROPERTY,OTHER_INSTALL,RESIDENCE,NUM_CREDITS, RESPONSE) #TELEPHONE


p2<- data_scale %>%  
      tidyr::gather(variable, value, c("DURATION", "AMOUNT", 
                                       "INSTALL_RATE", "NUM_CREDITS")) %>% 
      ggplot(aes(x = variable, y = value, fill = variable)) + 
      geom_boxplot() + 
      theme_bw() +
      coord_flip()+ 
      theme(legend.position="bottom")+
      theme(legend.title = element_blank())+ 
      theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
      labs(title = "Standardized variables",  
           x = "Variables", y = "Value (units)")

gridExtra::grid.arrange(p1, p2, ncol=2, nrow = 1)

rm(p1, p2)

##################quetion--> it is ok that we have negatives values##################

```

Now that the normalization is done, lets move on by creation of the training and test set based on the data.This will be done by dividing it in a randomly selection into the two subsets, with 75% of the data in the training set and the remaining 25% in the test set.

```{r, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

#so that we always have the same division
set.seed(2311)


#creation of the index to divide the data in the two subsets
val_index<-createDataPartition(data_scale$RESPONSE, 
                               p=0.75, list=FALSE)

#########training dataset
TrainData<-as.data.frame(data_scale[val_index,])

########test dataset
TestData <- data_scale[-val_index,]

####################graph############################################

#table for the graph of training and testing

Data_set<- c("Total", "Training", "Test")
Number_of_obs<- c(999, 750, 249)

p0<- gridExtra::grid.arrange(tableGrob(data.frame(Data_set,Number_of_obs))) 

p1<- ggplot2::ggplot(data=data_scale, aes(x= RESPONSE, fill=RESPONSE)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        theme_bw()+
        ylab("Freq")+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio- Data 100%",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")

p2<- ggplot2::ggplot(data=TrainData, aes(x= RESPONSE, fill=RESPONSE)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        theme_bw()+
        ylab("Freq")+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio- Data 75%",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")

p3<- ggplot2::ggplot(data=TestData, aes(x= RESPONSE, fill=RESPONSE)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        theme_bw()+
        ylab("Freq")+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio- Data 25%",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")

```

```{r, warning=FALSE, message=FALSE, echo=FALSE}

grid.arrange(p0, p1, p2, p3, ncol=2)

rm(p1, p0, p2, p3, Data_set, Number_of_obs) #delete variables


```

As you can see above, the data have the same proportion in the dataset, the training and the test set. Specifically, in all of them the dependent variable is biased, since it shows a greater tendency for a positive response. For this reason we will evaluate two fits for each algorithm, one with the skewed data and the other with a balanced one. Finally, to be able to compare them we are going to compute the confusion matrix, which includes the following information: 

- \[ Accuracy = \frac{TruePositive+TrueNegative} {TruePositive+TrueNegative+FalsePositive+FalseNegative}\] 
- \[ Sensitivity = \frac{TruePositive} {TruePositive+FalseNegative}\]
- \[ Specifity = \frac{TrueNegative} {TrueNegative+FalsePositive}\]

In synthesis, the sensitivity measures the true positive rate, which is key for this project, since a false positive has a negative impact on our main objective, as it would increase the risk of not being able to refund agreed payments. Meaning that in addition to balancing the data we will focus the second model on **maximizing sensitivity**.

### M1: Logistic regression

The general equation for the model is:

\[ Z_{i} = ln(\frac{P_{i}} {1-P_{i}}) = \beta_0+\beta_1X_1+...+\beta_nX_n \]

For the application of the algorithm we will apply the following steps:

| Data set        | Steps                                                       | 
| :--------------:|:-------------------------------------------------------------|
| Unbalanced data   | As we have sees earlier, the output variable is unbalanced. We are going to evaluate the accuracy and the sensitivity of the model, with the following steps: <br /> 1) Fit the model. <br /> 2) Coefficient analysis <br /> 3) Predict . <br /> 4) Confusion matrix .| 
| Balanced data | In this step, we are going to balance the data with the *training.control* function and, then, we will evaluate the accuracy and the sensitivity of the model, with the following steps: <br /> 5) Fit the model. <br /> 6) Predict <br /> 7) Confusion matrix |

#### Unbalanced data {.tabset .tabset-fade .tabset-pills}


##### 1) Fitting the model

```{r, echo=TRUE, warning=FALSE, message=FALSE}

#Same division
set.seed(1234)

#########################model######################################

train_params <- caret::trainControl(method = "repeatedcv", number = 10, repeats=5) 
#10-Fold Cross Validation   #5 repetitions

mod_lg_fit <- caret::train(RESPONSE ~ ., TrainData, method="glm", 
                           family="binomial",trControl= train_params)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

################check outputs################################
summary(mod_lg_fit)

```

In this step, we can see that the variables that take the highest importance and that are statistically significant for the model are: the second and third level of `CHK_ACC`, `DURATION`, the fourth level of `HISTORY`,the first level of `PURPOSE`, `AMOUNT`,the fourth level of `SAV_ACCT` and the third level of `EMPLOYMENT` and the first level of `OTHER_INSTALL`.

##### 2) Coefficients 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

##variable is significance--> high coefficient = bring high information

temp <- summary(mod_lg_fit)$coeff[-1,4] < 0.05

############condition: yes or not#####################################

temp %<>% as.data.frame()  

kableExtra::kable(temp, caption = "Significance of variable")%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

#remove variable temp
rm(temp) 

#########################################################################

#mod_lg_fit$coefnames

#########################################################################

```

If we look at the coeffiecients of the different variables we can conclude that, among the significant one that we described before, `CHK_ACCT`, `HISTORY` (all but the first level), `SAV_ACCT`, `EMPLOYMENT` and `MALE_SINGLE`, have a positive impact on the output, meaning that the higher is their level, or if they are positive, the probability of having `RESPONSE` = 1 will increase. 

On the other hand, among the significant variables, `DURATION`, `PURPOSE` (all but level two and four), `AMOUNT` and `OTHER_INSTALL` have a negative effect on the output, meaning that if they increase their level or value, or if they have a positive value (for the dummies), the probability of having a positive response will decrease.

The linear predictor is given by 
$$ \eta  = - 0.9 + 0.3 * CHKACCT_1 + 1.2 * CHKACCT_2  + 1.7 * CHKACCT_3 - 0.3 * DURATION - <br /> 0.4 * HISTORY_1 + 0.5 * HISTORY_2 +  0.7 * HISTORY_3 + 1.4 * HISTORY_4 - <br /> 0.9 * PURPOSE_1 + 0.8 * PURPOSE_2 - 0.08 * PURPOSE_3 + 0.05 * PURPOSE_4 - <br />0.7 * PURPOSE_5 - 0.08 * PURPOSE_6 - 0.3 * AMOUNT - 0.5 * SAVACCT_1 + 0.2 * SAVACCT_2 + <br /> 0.7 * SAVACCT_3 + 1.3 * SAVACCT_4 + 0.25 * EMPLOYMENT_1 + * 0.6 EMPLOYMENT_2 +<br /> 1.1 * EMPLOYMENT_3 + 0.7 * EMPLOYMENT_4 - 0.4 * INSTALLRATE + 0.5 * MALESINGLE_1 +<br /> 0.9 * GUARANTOR_1 + 0.06 * PROPERTY_1 - 0.5 * PROPERTY_2 - 0.6 * OTHERINSTALL_1 -<br /> 0.8 * RESIDENCE_1 - 0.3 * RESIDENCE_2 - 0.1 * NUMCREDITS + 0.4426 * TELEPHONE_1 $$

To be clear, if for example the purpose variable takes value 3, only the coefficient of PURPOSE_3 will be added to the others. The same goes for each other categorical variable. For the dummies the coefficient is added only if the value is equal to 1 for the variable, otherwise no. While for the continuous variables the coefficient is multiplied by the value that is recorded in the observation.

##### 3) Prediction 

Now we will get the predictions using this model. To being able to do it, we will start by getting the probabilities of the output given the coefficients we have found by fitting the model, then we will use a cut point of 0.5 to decide whether the value will be equal to 1 (if the probability it higher than 0.5) or 0 (otherwise).
The model basically fit the information of the new observation in the function that is given above, and then it finds a value *eta* that is then used to get the prediction of the probability of the output by doint p = 1 / (1 + eta), which we will use to determine the class predicted for the outcome.

```{r, echo=TRUE, message=FALSE, warning=FALSE}

#prediction given the model
lg.pred <- predict(mod_lg_fit, newdata = TestData)  

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model

plot_pred_unbalance <- as.data.frame(as.vector(lg.pred))
names(plot_pred_unbalance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio- testing unbalance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")+
        theme_bw()
p

rm(plot_pred_unbalance, p)

```

The unbalance towards the positive value prediction is more than clear, in this graph. 

##### 4) Diagnosis 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(lg.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

#### save the variables#######

sens<- caret::sensitivity(table(as.factor(lg.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(lg.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]

logistic<- c(sens , sp, acc)
logistic<-round(as.numeric(logistic),2)

#############remove temporary element
rm(cm, sens, sp, acc)

```

The sensitivity is quite low, at 52%, the specificity, though, is high, almost 90%, while the accuracy is at 78%. The number of false positive is quite high, being 36.

#### {.toc-ignore}

#### Balanced data {.tabset .tabset-fade .tabset-pills}

##### 5) Fitting the model

```{r, echo=TRUE, warning=FALSE, message=FALSE}

#Same division
set.seed(1234)

#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv", number = 10, 
                                    repeats=5, sampling = "down")

mod_lg_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="glm", 
                                  family="binomial", 
                                  metric = "Sens", #optimize sensitivity
                                  maximize = TRUE, #maximize the metric
                                  trControl= train_params)

################check outputs################################vv
summary(mod_lg_fitbalance)

```

##### 6) Prediction

```{r, echo=TRUE, message=FALSE, warning=FALSE}

#probability given the model
lg.pred.b <- predict(mod_lg_fitbalance, newdata = TestData)  

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(lg.pred.b))
names(plot_pred_balance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio- testing balance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")+
        theme_bw()
p

rm(plot_pred_balance, p)

```

Contrary to the output of the first model, we can see that the proportion of the prediction is better in the balanced case.

##### 7) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(lg.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

#### save the variables#######

sens.b<- caret::sensitivity(table(as.factor(lg.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(lg.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]

logistic_balance<- c(sens.b , sp.b, acc.b)
logistic_balance<-round(as.numeric(logistic_balance),2)

#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)

```

The sensitivity is 72%, the specificity is 64% and the accuracy only 66%.  

#### {.toc-ignore}



### M2: Decision trees

The next image illustrates better the way of working of decision trees.

```{r, echo=FALSE, fig.cap="A caption", out.width = '100%', warning=FALSE, message=FALSE}

library(here)
library(knitr)
#we take this picture from here: 
#https://okanbulut.github.io/bigdata/supervised-machine-learning-part-i.html

knitr::include_graphics(here::here("data/DT.png"))

```

The process consists in the minimization of the classification error rate:

\[E=\ 1-max_{k}(p_{mk})\] where $p_{mk}$ is the proportion of training observation.


For the application of the algorithm we will apply the following steps:

| Data set        | Steps                                                       | 
| :--------------:|:-------------------------------------------------------------|
| Unbalanced data   | As we have seen earlier, the output variable is unbalanced. We are going to evaluate the accuracy and the sensitivity of the model, with the following steps: <br /> 1) Fit the model. <br /> 2) Plot the best tree <br /> 3) Predict . <br /> 4) Confusion matrix .| 
| Balanced data | In this step, we are going to balance the data with the *training.control* function and, then, we will evaluate the accuracy and the sensitivity of the model, with the following steps: <br /> 5) Fit the model. <br /> 6) Plot the best tree <br /> 6) Predict <br /> 7)
Confusion matrix |

#### Unbalanced Data {.tabset .tabset-fade .tabset-pills}

##### 1) Fitting the model

We will start by fitting the model on the data.

```{r, echo=TRUE, message=FALSE, warning=FALSE}

#Same division
set.seed(1234)

#########################model######################################
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5) 

#10-Fold Cross Validation #5 repetions

mod_dt_fit <- caret::train(RESPONSE ~ ., TrainData, method="rpart", 
                           trControl= train_params)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

################check outputs################################
print(mod_dt_fit)

```

##### 2) Plot 

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.asp=1.5}

#######plot #########################
suppressMessages(library(rattle))
rattle::fancyRpartPlot(mod_dt_fit$finalModel)

```

##### 3) Prediction 

```{r, echo=TRUE, message=FALSE, warning=FALSE}

#prediction given the model
dt.pred <- predict(mod_dt_fit, newdata = TestData)  #predict give me the probability i am looking for the the binomial answer

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(dt.pred))
names(plot_pred_unbalance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio DT - testing balance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")+
        theme_bw()
p

rm(plot_pred_unbalance, p)

```

The prediction is clearly biased to a positive answer.

##### 4) Diagnosis 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(dt.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

#############Parameters

sens<- caret::sensitivity(table(as.factor(dt.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(dt.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]

decision_tree<- c(sens , sp, acc)
decision_tree<-round(as.numeric(decision_tree),2)

#############remove temporary element
rm(cm, sens, sp, acc)


```

We can see that here the sensitivity is really low, while the specificity is higher, reaching a value above 92%, which is in any case the one in which we are the most interested. The accuracy is around 74%. Here, in 70 cases in which the model should have given a negative value, it actually predicted a positive one, and it could cost quite a lot to the company.

#### {.toc-ignore}

#### Balanced data {.tabset .tabset-fade .tabset-pills}

##### 5) Fitting the model

```{r, echo=TRUE, warning=FALSE, message=FALSE}

#Same division
set.seed(1234)

#########################model######################################

train_params <- caret::trainControl(method = "repeatedcv", number = 10, 
                                    repeats=5, sampling = "down")

mod_dt_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="rpart", 
                                  metric = "Sens", #optimize sensitivity
                                  maximize = TRUE, #maximize the metric
                                  trControl= train_params)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

################check outputs################################
print(mod_dt_fitbalance)

```

##### 6) Plot

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#######plot option 1 #########################
##plot(mod_dt_fitbalance$finalModel, uniform=TRUE,
##     main="Classification Tree")
##text(mod_dt_fitbalance$finalModel, use.n.=TRUE, all=TRUE, cex=.8)


#######plot option 2 #########################
suppressMessages(library(rattle))
rattle::fancyRpartPlot(mod_dt_fitbalance$finalModel)


```

##### 7) Prediction

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#probability given the model
dt.pred.b <- predict(mod_dt_fitbalance, newdata = TestData)  #predict give me the probability i am looking for the the binomial answer

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(dt.pred.b))
names(plot_pred_balance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio DT - testing balance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")+
        theme_bw()
p

rm(plot_pred_balance, p)

```

We can see that it is a bit more balanced, even if the number of positive predictions is still higher. 

##### 8) Diagnosis 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(dt.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#############Parameters

sens.b<- caret::sensitivity(table(as.factor(dt.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(dt.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]

decision_tree_balance<- c(sens.b , sp.b, acc.b)
decision_tree_balance<-round(as.numeric(decision_tree_balance),2)

#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)


```

We can see that here the sensitivity has improved, however the specificity is lower, reaching a value above 70%, which is in any case the one in which we are the most interested. The accuracy is around 62%. 

#### {.toc-ignore}

### M3: Discriminate analysis

There are four types of discriminate analysis, we will explain them in the following table:

| n° | Model  |  Definition                                                       | 
|:--:|:-------|:------------------------------------------------------------------|   
| 1  |  LDA   | > Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. <br /> (https://en.wikipedia.org/wiki/Linear_discriminant_analysis) |
| 2  | QDA   | >This method assume that the measurements from each class are normally distributed, but there is not assumption saying that the covariance of each of the classes is identical.When the normality assumption is true, the best possible test for the hypothesis that a given measurement is from a given class is the likelihood ratio test.<br /> (https://en.wikipedia.org/wiki/Quadratic_classifier) |
| 3  | FDA   |  > It analyzes data providing information about curves, surfaces or anything else varying over a continuum. In its most general form, under an FDA framework each sample element is considered to be a function.<br /> (https://en.wikipedia.org/wiki/Functional_data_analysis)|
| 4  | MDA   | > It is a multivariate dimensionality reduction technique. It has been used to predict signals as diverse as neural memory traces and corporate failure. <br /> (https://en.wikipedia.org/wiki/Multiple_discriminant_analysis)|

The next image illustrates better the way of working for each model.

```{r, echo=FALSE,fig.cap="Principal difference between each model", paged.print=FALSE, out.width = '70%', warning=FALSE, message=FALSE}

library(here)
library(knitr)
#we take this picture from here: 
#http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/

knitr::include_graphics(here::here("data/LDA-QDA-MDA.png"))

```


#### Linear Discriminant Analysis 

Steps for the application of the algorithm:

| Data set        | Steps                                                       | 
| :--------------:|:-------------------------------------------------------------|
| Unbalanced data |  1) Fit the model. <br /> 2) Predict . <br /> 3) Confusion matrix .| 
| Balanced data   | 4) Fit the model. <br /> 5) Predict <br /> 6) Confusion matrix |


#### Unbalanced {.tabset .tabset-fade .tabset-pills}

##### 1) Fitting the model

```{r, echo=TRUE, message=FALSE, warning=FALSE}

#Same division
set.seed(1234)

#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv", number = 10, repeats=5) 
#K-Fold Cross Validation
mod_lda_fit <- caret::train(RESPONSE ~ ., TrainData, method="lda", 
                           family="binomial",trControl= train_params)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

################check outputs################################vv
summary(mod_lda_fit)

```

The linear combination of predictor variables that are used to form the decision rule is the following: 

$$ RESPONSE = -0.3265 * DURATION -0.2747 * HISTORY_1 + 0.8792 * HISTORY_2 + 1.1810 * HISTORY_3 + 1.6214 * HISTORY_4 - 0.7437 * PURPOSE_1 + 0.7736 * PURPOSE_2 + 0.0172 * PURPOSE_3 + 0.2035 * PURPOSE_4 - 0.6116 * PURPOSE_5 + 0.1298 * PURPOSE_6 - 0.2579 * AMOUNT + 0.5066 * SAV_ACCT_1 + 0.7517 * SAV_ACCT_2 + 0.7778 * SAV_ACCT_3 + 1.0997 * SAV_ACCT_4 + 0.6175 * EMPLOYMENT_1 + 1.1982 * EMPLOYMENT_2 + 1.4580 * EMPLOYMENT_3 + 1.1806 * EMPLOYMENT_4 - 0.2897 * INSTALL_RATE - 0.5663 * SEX_MALE_1 + 0.9272 * MALE_SINGLE_1 + 0.5183 * MALE_MAR_WID_1 - 0.0863 * CO_APPLICANT_1 + 0.5084 * GUARANTOR_1 - 0.2437 * PRESENT_RESIDENT_-1 - 0.1913 * PRESENT_RESIDENT_0 - 0.0477 * PRESENT_RESIDENT_1 + 0.0367 * PROPERTY_1 - 0.6374 * PROPERTY_2 + 0.0502 * AGE - 0.4501 * OTHER_INSTALL_1 - 0.7509 * RESIDENCE_1 - 0.2185 * RESIDENCE_2 - 0.0703 * NUM_CREDITS - 1.0570 * JOB_1 - 1.0581 * JOB_2 - 0.8362 * JOB_3$$

Each new observation will be evaluated thanks to this formula, with its information put inside of it. It follows the same principle described for the generalized linear model. 

##### 2) Prediction

```{r, echo=TRUE, message=FALSE, warning=FALSE}

lda.pred <- predict(mod_lda_fit, newdata = TestData)  #predict give me the probability i am looking for the the binomial answer

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(lda.pred))
names(plot_pred_unbalance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio LDA - Testing unbalance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")

p


rm(plot_pred_unbalance, p)

```

With this graph we confirm what has been explained in the fit part, which is the prediction tending to give a positive response. 

##### 3) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(lda.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#############
sens<- caret::sensitivity(table(as.factor(lda.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(lda.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]

lda<- c(sens , sp, acc)
lda<-round(as.numeric(lda),2)

#############remove temporary element
rm(cm, sens, sp, acc)

```

Here the sensitivity is higher with respect to the previous unbalance models (above 40%), but it is still quite low. If we look at the accuracy, is quite low, as it is only around 78%. What is important to note is that 48 times in which the model would have predicted a positive value for the output, it should have been negative, which is something that could cost quite a lot to the copmany. 

#### {.toc-ignore}

#### Balanced data {.tabset .tabset-fade .tabset-pills}

##### 4) Fitting the model

```{r, echo=TRUE, warning=FALSE, message=FALSE}

#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv", number = 10, 
                                    repeats=5, sampling = "down") 


mod_lda_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="lda", 
                           family="binomial",
                           metric = "Sens", #optimize sensitivity
                           maximize = TRUE, #maximize the metric
                           trControl= train_params)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

################check outputs################################vv
summary(mod_lda_fitbalance)

```



##### 5) Prediction

```{r, echo=TRUE, message=FALSE, warning=FALSE}

lda.pred.b <- predict(mod_lda_fitbalance, newdata = TestData)  #predict give me the probability i am looking for the the binomial answer

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(lda.pred.b))
names(plot_pred_balance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio LDA - testing balance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_balance, p)

```

We can see that the situation is more balanced.

##### 6) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(lda.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

#########

sens.b<- caret::sensitivity(table(as.factor(lda.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(lda.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]

lda_balance<- c(sens.b , sp.b, acc.b)
lda_balance<-round(as.numeric(lda_balance),2)

#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)

```

The sensitivity is 73%, the specificity is only 69% and the accuracy is also low, at 67%. The number of false positive, though, is only 26 within 100 request. 

#### {.toc-ignore}


#### Quadratic discriminant analysis 

Steps for the aplication of the algorithm:

| Data set        | Steps                                                       | 
| :--------------:|:-------------------------------------------------------------|
| Unbalanced data |  1) Fit the model. <br /> 2) Predict . <br /> 3) Confusion matrix .| 
| Balanced data   | 4) Fit the model. <br /> 5) Predict <br /> 6) Confusion matrix |

#### Unbalanced data {.tabset .tabset-fade .tabset-pills}

##### 1) Fitting the model

```{r, echo=TRUE, message=FALSE, warning=FALSE}

#Same division
set.seed(1234)

#########################model######################################vvvv
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5) #K-Fold Cross Validation
mod_qda_fit <- caret::train(RESPONSE ~ ., TrainData, method="qda", 
                           family="binomial",trControl= train_params)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

################check outputs################################
summary(mod_qda_fit)

```

##### 2) Prediction

```{r, echo=FALSE, message=FALSE, warning=FALSE}

qda.pred <- predict(mod_qda_fit, newdata = TestData)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(qda.pred))
names(plot_pred_unbalance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio QDA - testing unbalance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_unbalance, p)

```

Here, it seems still that the majority of the false prediction are in the positive level, however they seem less than before.

##### 3) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(qda.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

#############
sens<- caret::sensitivity(table(as.factor(qda.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(qda.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]

qda<- c(sens , sp, acc)
qda<-round(as.numeric(qda),2)

#############remove temporary element
rm(cm, sens, sp, acc)

```

The sensitivity is 54%, the specificity is high, reaching almost 84%, and the accuracy is 75%. The number of false positive, however, is around 46. 

#### {.toc-ignore}

As we predicted, the model performs a little bit worse than the LDA, but for the sensitivity, which is the highest up to now (over 50%), is still quite low, though. The specificity is moderately high (above 80%) as it is the accuracy (above 70%). As we want to have a value for the false positive low, the 34 here is still quite high.

#### Balanced data {.tabset .tabset-fade .tabset-pills}

##### 4) Fitting the model

```{r, echo=FALSE, warning=FALSE, message=FALSE}

#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv", number = 10, 
                                    repeats=5, sampling = "down")


mod_qda_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="qda", 
                           family="binomial",
                           metric = "Sens", #optimize sensitivity
                           maximize = TRUE, #maximize the metric
                           trControl= train_params)

```

##### 5) Prediction

```{r, echo=TRUE, message=FALSE, warning=FALSE}

qda.pred.b <- predict(mod_qda_fitbalance, newdata = TestData)  

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(qda.pred.b))
names(plot_pred_balance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio QDA - testing balance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_balance, p)

```

We can see that there is still some unbalance towards the positive value.

##### 6) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(qda.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#########

sens.b<- caret::sensitivity(table(as.factor(qda.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(qda.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]

qda_balance<- c(sens.b , sp.b, acc.b)
qda_balance<-round(as.numeric(qda_balance),2)

#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)

```

The sensitivity is 65%, the specificity is 71% and the accuracy almost 70%, while the false positive will be around 34 withing 100 request. 

#### {.toc-ignore}

#### Functional data analysis (FDA) 

Steps for the application of the algorithm:

| Data set        | Steps                                                       | 
| :--------------:|:-------------------------------------------------------------|
| Unbalanced data |  1) Fit the model. <br /> 2) Predict . <br /> 3) Confusion matrix .| 
| Balanced data   | 4) Fit the model. <br /> 5) Predict <br /> 6) Confusion matrix |

#### Unbalanced data {.tabset .tabset-fade .tabset-pills}

##### 1) Fitting the model

```{r, echo=TRUE, message=FALSE, warning=FALSE}

#Same division
set.seed(1234)

#########################model######################################vvvv
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5) #K-Fold Cross Validation

library(earth)
mod_fda_fit <- caret::train(RESPONSE ~ ., TrainData, method="fda", 
                              trControl= train_params)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

################check outputs################################vv
mod_fda_fit

```

##### 2) Prediction

```{r, echo=TRUE, message=FALSE, warning=FALSE}

fda.pred <- predict(mod_fda_fit, newdata = TestData)  

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(fda.pred))
names(plot_pred_unbalance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio FDA - testing unbalance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_unbalance, p)

```

The unbalance towards the positive value is more than clear in this graph.

##### 3) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(fda.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

#############
sens<- caret::sensitivity(table(as.factor(fda.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(fda.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]

fda<- c(sens , sp, acc)
fda<-round(as.numeric(fda),2)

#############remove temporary element
rm(cm, sens, sp, acc)

```

This model has a sensitivity of almost 55%, among one of the highest up to now, and the specificity is higher than 85%. The accuracy is around 76%. The false positive observations are 46.

#### {.toc-ignore}

#### Balanced data {.tabset .tabset-fade .tabset-pills}

##### 4) Fitting the model

```{r, echo=TRUE, warning=FALSE, message=FALSE}

#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv", number = 10, 
                                    repeats=5, sampling = "down")

mod_fda_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="fda", 
                                   metric = "Sens", #optimize sensitivity
                                    maximize = TRUE,
                                    trControl= train_params)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

################check outputs################################vv
summary(mod_fda_fitbalance)

```

##### 5) Prediction

Using the model we get the predictions for the `RESPONSE` variable and we can construct the confidence matrix for this case.

```{r, echo=TRUE, message=FALSE, warning=FALSE}

fda.pred.b <- predict(mod_fda_fitbalance, newdata = TestData)  #predict give me the probability i am looking for the the binomial answer

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(fda.pred.b))
names(plot_pred_balance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio FDA - testing balance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_balance, p)

```

We can see that the situation is more balanced. 

##### 6) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(fda.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

#####################

sens.b<- caret::sensitivity(table(as.factor(fda.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(fda.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]

fda_balance<- c(sens.b , sp.b, acc.b)
fda_balance<-round(as.numeric(fda_balance),2)

#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)

```

Here, the sensitivity is almost 79%, while the specificity is 66%, with an accuracy of almost 70%. The false positive are really low, reaching 22 observations within 100 request. 

#### {.toc-ignore}


#### Mixture discriminant analysis (MDA) {.tabset .tabset-fade .tabset-pills}

Steps for the application of the algorithm:

| Data set        | Steps                                                       | 
| :--------------:|:-------------------------------------------------------------|
| Unbalanced data |  1) Fit the model. <br /> 2) Predict . <br /> 3) Confusion matrix .| 
| Balanced data   | 4) Fit the model. <br /> 5) Predict <br /> 6) Confusion matrix |

#### Unbalanced data {.tabset .tabset-fade .tabset-pills}

##### 1) Fitting the model

```{r, echo=TRUE, message=FALSE, warning=FALSE}

#Same division
set.seed(1234)

#########################model######################################vvvv
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5) #K-Fold Cross Validation
mod_mda_fit <- caret::train(RESPONSE ~ ., TrainData, method="mda", 
                           family="binomial",trControl= train_params)

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

################check outputs################################vv
summary(mod_mda_fit)

```

##### 2) Prediction 

```{r, echo=TRUE, message=FALSE, warning=FALSE}

mda.pred <- predict(mod_mda_fit, newdata = TestData)  

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(mda.pred))
names(plot_pred_unbalance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio MDA - testing unbalance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_unbalance, p)

```

The unbalance towards the positive value is more than clear in this graph. 

##### 3) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(mda.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#############remove temporary element
sens<- caret::sensitivity(table(as.factor(mda.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(mda.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]

mda<- c(sens , sp, acc)
mda<-round(as.numeric(mda),2)

#############remove temporary element
rm(cm, sens, sp, acc)

```

The sensitivity in this case is around 55%, while the specificity is higher, reaching 85%. The accurary is around 75% and there are 46 false positive (from 100 request).

#### {.toc-ignore}

#### Balanced data {.tabset .tabset-fade .tabset-pills}

##### 4) Fitting the model

```{r, echo=TRUE, warning=FALSE, message=FALSE}

#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv", number = 10, 
                                    repeats=5, sampling = "down")

mod_mda_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="mda", 
                           family="binomial",
                           metric = "Sens", #optimize sensitivity
                           maximize = TRUE,
                           trControl= train_params)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

################check outputs################################vv
mod_mda_fitbalance

```

##### 5) Prediction

```{r, echo=TRUE, message=FALSE, warning=FALSE}

mda.pred.b <- predict(mod_mda_fitbalance, newdata = TestData)  #predict give me the probability i am looking for the the binomial answer

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(mda.pred.b))
names(plot_pred_balance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio MDA - testing balance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_balance, p)

```

We have still some unbalance toward the positive value. 

##### 6) Diagnosis 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(mda.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

#########

sens.b<- caret::sensitivity(table(as.factor(mda.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(mda.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]

mda_balance<- c(sens.b , sp.b, acc.b)
mda_balance<-round(as.numeric(mda_balance),2)

#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)

```

Here the sesntitivty is 72%, the specificity 74% and the accuracy 73%. The false positive are decreasing, with a value of 28%. 

#### {.toc-ignore}

### M4: Random Forest

Steps for the application of the algorithm:

| Data set        | Steps                                                       | 
| :--------------:|:-------------------------------------------------------------|
| Unbalanced data |  1) Fit the model. <br /> 2)Checking Variables. <br /> 3) Predict . <br /> 4) Confusion matrix .| 
| Balanced data   | 5) Fit the model. <br /> 6) Predict <br /> 7) Confusion matrix |

#### Unbalanced data {.tabset .tabset-fade .tabset-pills}

##### 1) Fitting the model

```{r, echo=TRUE, message=FALSE, warning=FALSE}

#Same division
set.seed(1234)

#########################model######################################vvvv
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5) #K-Fold Cross Validation
mod_rf_fit <- caret::train(RESPONSE ~ ., TrainData, method="rf", 
                           trControl= train_params)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

################check outputs################################vv
print(mod_rf_fit)

```

The summary of the model gives the decrease in accuracy and the decrease of the gini index for each variable in the model, along with the number of trees that are built (500 in our case), the number of variabes that are randomly chosen to be tried at each split before determining which one is the best one to describe the node. Moreover, we can already find the confusion matrix (we will show it better again afterwards to keep the coherence of the analysis throuhgout all the models), with the class errorand the Out-Of-Bag estimate of the error rate. 

Let's give some definitions to be clearer:

> Variable importance is the mean decrease of accuracy over all out-of-bag cross validated predictions, when a given variable is permuted after training, but before prediction.

> GINI importance measures the average gain of purity by splits of a given variable. If the variable is useful, it tends to split mixed labeled nodes into pure single class nodes. Splitting by a permuted variables tend neither to increase nor decrease node purities. 

source:https://stats.stackexchange.com/questions/197827/how-to-interpret-mean-decrease-in-accuracy-and-mean-decrease-gini-in-random-fore

> Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging) to sub-sample data samples used for training. OOB is the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample.
source: https://en.wikipedia.org/wiki/Out-of-bag_error

##### 2) Checking Variables

```{r, echo=FALSE, fig.asp=1.5}

#Variable importance analysis
rf.fit <- randomForest::randomForest(RESPONSE ~., 
                       TrainData, 
                       ntree = 500, 
                       mtry = 4, 
                       importance = TRUE, 
                       replace = FALSE)

randomForest::importance(rf.fit) 
randomForest::varImpPlot(rf.fit, main ='Variable Importance')

#two criteria: MeanDecreaseAccuracy (rough estimate of the loss in prediction performance when that particular variable is omitted from the training set) and Mean Decrease Gini (GINI is a measure of node impurity, highest purity means that each node contains only elements of a single class. Assessing the decrease in GINI when that feature is omitted leads to an understanding of how important that feature is to split the data correctly)

```

The most important variables appear to be CHK_ACC, DURATION and HISTORY in terms of Accuracy and AMOUNT, CHK_ACC and DURATION in terms of gini index, which is consistent with what we have found up to now. 

##### 3) Prediction 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

rf.pred <- predict(mod_rf_fit, newdata = TestData)  #predict give me the probability i am looking for the the binomial answer

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(rf.pred))
names(plot_pred_unbalance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio Random Forest - testing unbalance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_unbalance, p)

```

The predictions shows a clear preference towards the positive value.

##### 4) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(rf.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
############
sens<- caret::sensitivity(table(as.factor(rf.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(rf.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]

rf<- c(sens , sp, acc)
rf<-round(as.numeric(rf),2)

#############remove temporary element
rm(cm, sens, sp, acc)

```

Here the sensitivity is around 46%, the specificity is high (more than 87%) and the accuracy is around 75%, while the number of false positive is 54 from 100 request. 

#### {.toc-ignore}

#### Balanced data {.tabset .tabset-fade .tabset-pills}

##### 5) Fitting the model

```{r, echo=TRUE, warning=FALSE, message=FALSE}

train_params <- caret::trainControl(method = "repeatedcv", number = 10, 
                                    repeats=5, sampling = "down")

mod_rf_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="rf", 
                           family="binomial",
                           metric = "Sens", #optimize sensitivity
                           maximize = TRUE,
                           trControl= train_params)

```


```{r, echo=FALSE, warning=FALSE, message=FALSE}

################check outputs################################vv
print(mod_rf_fitbalance)

```

##### 6) Prediction

```{r, echo=TRUE, message=FALSE, warning=FALSE}

rf.pred.b <- predict(mod_rf_fitbalance, newdata = TestData)  

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(rf.pred.b))
names(plot_pred_balance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio Random Forest - testing balance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_balance, p)

```

We can see that the predictions are more balanced.

##### 7) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(rf.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
######################

sens.b<- caret::sensitivity(table(as.factor(rf.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(rf.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]

rf_balance<- c(sens.b , sp.b, acc.b)
rf_balance<-round(as.numeric(rf_balance),2)

#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)

```

Here the sensitivity is 78%, the specificity is 65% and the accuracy almost 70%. However the false positive are really low, being only 22 cases from 100 request. 

#### {.toc-ignore}

### M5: Neural Networks

Steps for the application of the algorithm:

| Data set        | Steps                                                        | 
| :--------------:|:-------------------------------------------------------------|
| Unbalanced data |  1) Fit the model. <br /> 2) Plot <br /> 3) Predict . <br /> 4) Confusion matrix .| 
| Balanced data   | 5) Fit the model. <br /> 6) Plot<br /> 7) Predict <br /> 8) Confusion matrix |

#### Unbalanced data {.tabset .tabset-fade .tabset-pills}

##### 1) Fitting the model

```{r, echo=TRUE, message=FALSE, warning=FALSE, results="hide"}

#Same division
set.seed(1234)

#########################model######################################
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5) 
mod_nn_fit <- caret::train(RESPONSE ~ ., TrainData, method="nnet", 
                           trControl= train_params)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

################check outputs################################
print(mod_nn_fit)

```

##### 2) Plot

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.asp=2}

NeuralNetTools::plotnet(mod_nn_fit$finalModel, y_names = "yes/no")
title("Graphical Representation of our Neural Network")

```

##### 3) Prediction

```{r, echo=TRUE, message=FALSE, warning=FALSE}

nn.pred <- predict(mod_nn_fit, newdata = TestData)  

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(nn.pred))
names(plot_pred_unbalance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio NN - testing unbalance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_unbalance, p)

```

We can see an unblanced result toward the positive value.

##### 4) Diagnosis 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(nn.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

#############

sens<- caret::sensitivity(table(as.factor(nn.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(nn.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]

nn<- c(sens , sp, acc)
nn<-round(as.numeric(nn),2)

#############remove temporary element
rm(cm, sens, sp, acc)

```

Here, the sensitivity is 53%, but the specificity is almost 88%, with an accuracy of 77%. The false positive, however, are still quite high around 47 of 100 requests that are accepted will be incorrect. 

#### {.toc-ignore}

#### Balanced data {.tabset .tabset-fade .tabset-pills}

##### 5) Fitting the model

```{r, echo=TRUE, warning=FALSE, message=FALSE, results="hide"}

train_params <- caret::trainControl(method = "repeatedcv", number = 10, 
                                    repeats=5, sampling = "down")

mod_nn_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="nnet", 
                           family="binomial",
                           metric = "Sens", #optimize sensitivity
                           maximize = TRUE,
                           trControl= train_params)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

################check outputs################################
print(mod_nn_fitbalance)

```

##### 6) Prediction

```{r, echo=FALSE, message=FALSE, warning=FALSE}

nn.pred.b <- predict(mod_nn_fitbalance, newdata = TestData)  

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(nn.pred.b))
names(plot_pred_balance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio NN - testing balance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_balance, p)

```

##### 7) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(nn.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

######################

sens.b<- caret::sensitivity(table(as.factor(nn.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(nn.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]

nn_balance<- c(sens.b , sp.b, acc.b)
nn_balance<-round(as.numeric(nn_balance),2)

#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)

```

Here we see a sesitivity of 77%, a really high specificity among the ones we have found (61%) and an accuracy of only 66%. Moreover, the false positive are quite low, being only 22 from 100 requested accepted.

#### {.toc-ignore}

### M6: XGBoost

Steps for the application of the algorithm:

| Data set        | Steps                                                       | 
| :--------------:|:-------------------------------------------------------------|
| Unbalanced data |  1) Fit the model. <br /> 2) Plot  <br /> 3) Predict . <br /> 3) Confusion matrix .| 
| Balanced data   | 4) Fit the model. <br /> 5) Predict <br /> 6) Confusion matrix |

#### Unbalanced data {.tabset .tabset-fade .tabset-pills}

##### 1) Fitting the model

```{r, echo=TRUE, message=FALSE, warning=FALSE, results="hide"}

######################### transform data ############
data_xgboost <- purrr::map_df(data_scale, function(columna) {
                          columna %>% 
                          as.factor() %>% 
                          as.numeric %>% 
                          { . - 1 } })

test_xgboost <- sample_frac(data_xgboost, size = 0.249)
train_xgboost <- setdiff(data_xgboost, test_xgboost)


#Convertir a DMatrix

train_xgb_matrix <-   train_xgboost %>% 
                            dplyr::select(- RESPONSE) %>% 
                            as.matrix() %>% 
                            xgboost::xgb.DMatrix(data = ., label = train_xgboost$RESPONSE)
#Convertir a DMatrix

test_xgb_matrix <-  test_xgboost %>% 
                            dplyr::select(- RESPONSE) %>% 
                            as.matrix() %>% 
                            xgboost::xgb.DMatrix(data = ., label = test_xgboost$RESPONSE)

#Same division
set.seed(1234)

#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv", 
                             number = 10, # with n folds 
                             repeats=5) #K-Fold Cross Validation

mod_xgb_fit <- caret::train(RESPONSE ~ ., TrainData, 
                           method="xgbTree", 
                           trControl= train_params)

```

##### 2) Best model parameters

```{r, echo=FALSE, message=FALSE, warning=FALSE}

mod_xgb_fit$bestTune

```

##### 3) Prediction

```{r, echo=TRUE, message=FALSE, warning=FALSE}

xgb.pred <- predict(mod_xgb_fit, newdata = TestData)  

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(xgb.pred))
names(plot_pred_unbalance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio XGB - testing unbalance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_unbalance, p)

```


##### 4)  Diagnosis 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(xgb.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

######################

sens<- caret::sensitivity(table(as.factor(xgb.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(xgb.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]

xgb<- c(sens , sp, acc)
xgb<-round(as.numeric(xgb),2)

#############remove temporary element
rm(cm, sens, sp, acc)

```

Here the sensitivity is quite low, 53%, the specificity is at 87% though and the accuracy at 77%. There is no significant improvement over the other models as we could expect. 

#### {.toc-ignore}

#### Balanced data {.tabset .tabset-fade .tabset-pills}

##### 5) Fitting the model

```{r, echo=TRUE, warning=FALSE, message=FALSE, results="hide"}

train_params <- caret::trainControl(method = "repeatedcv", number = 10, 
                                    repeats=5, sampling = "down")

mod_xgb_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="xgbTree", 
                            metric = "Sens", #optimize sensitivity
                           maximize = TRUE,
                           trControl= train_params)

```

##### 6) Prediction

```{r, echo=TRUE, message=FALSE, warning=FALSE}

xgb.pred.b <- predict(mod_xgb_fitbalance, newdata = TestData) 

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(xgb.pred.b))
names(plot_pred_balance)<- c("predictions")

#table for the graph of training and testing

p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
        geom_bar(aes(y = (..count..)/sum(..count..)))+
        scale_y_continuous(labels=scales::percent) +
        ylab("Freq")+
        theme_bw()+
        theme(legend.title = element_blank()) + 
        theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
        labs(title = "Response Ratio XGB - testing balance",  
           x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p

rm(plot_pred_balance, p)

```

We can see that it is indeed a bit more balanced, however there is still a majoritiy of positive values predicted. 

##### 7) Diagnosis

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#############confusion matrix 
cm <- confusionMatrix(as.factor(xgb.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)

######################

sens.b<- caret::sensitivity(table(as.factor(xgb.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(xgb.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]

xgb_balance<- c(sens.b , sp.b, acc.b)
xgb_balance<-round(as.numeric(xgb_balance),2)

#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)


```

The sensitivity is almost 67%, the specificity is at 70% and the accuracy is almost 70%. The number of false positive is still at 34 from 100 accepted request.  

#### {.toc-ignore}

## Assess model

For this section we will measure the main parameters of the six analyzed models.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

Details<- c("Sensitivity", "Specificity", "Accuracy")
summary_table<- data.frame(Details, logistic, logistic_balance, 
                           decision_tree, decision_tree_balance,
                           lda, lda_balance, 
                           qda, qda_balance,
                           fda, fda_balance, 
                           mda, mda_balance,
                           rf, rf_balance,
                           nn, nn_balance,
                           xgb, xgb_balance)
                           

summary_table<-data.frame(t(summary_table[-1]))                           
names(summary_table) <- c("Sensitivity", "Specificity", "Accuracy")                          
                           
kable(summary_table, 
      caption = "Summary table for assess the model",
                        align = "c")%>%
kableExtra::kable_styling("striped") %>%
  pack_rows("Logistic Regression", 1, 2 , 
            label_row_css = "background-color: #666; color: #fff;") %>%
  pack_rows("Decision Trees", 3, 4, 
            label_row_css = "background-color: #666; color: #fff;")%>%
  pack_rows("DA: Linear Discriminant", 5, 6,
            label_row_css = "background-color: #666; color: #fff;")%>%
  pack_rows("DA: Quadratic discriminant analysis", 7, 8,
            label_row_css = "background-color: #666; color: #fff;")%>%
  pack_rows("DA: Functional data analysis", 9, 10, 
            label_row_css = "background-color: #666; color: #fff;") %>%
  pack_rows("DA: Mixture discriminant analysis", 11, 12, 
            label_row_css = "background-color: #666; color: #fff;") %>%
  pack_rows("Random Forest", 13, 14, 
            label_row_css = "background-color: #666; color: #fff;") %>%
  pack_rows("Neural Networks", 15, 16, 
            label_row_css = "background-color: #666; color: #fff;") %>%
  pack_rows("XGboost", 17, 18, 
            label_row_css = "background-color: #666; color: #fff;") %>%
  row_spec(14, bold = T, color = "white", background = "#D7261E")


```

We can see that in temrs of sensitivity, the best models are the balanced FDA, Random Forest and Neural Networks, in terms of specificity the unbalanced decision tree, LDA and logistic regression (we were expecting the unbalanced version to perform better in terms of specificity as we have a majority of positive values in the predictions, hence there will be more positive value predicted which will increase the value of the specificity), while in terms of accuracy the unbalanced LDA, logistic and Random Forest (same reasoning as for the specificity). 
We were quite surprised by the results we have found, we were expecting the XGB to perform better then the random forest, but we can see that it is actually giving lower values both for specificity and accuracy in the unbalanced case, while the balanced case is only worse in terms of sensitivity compared to the balanced random forest. 

More details on the evaluation of the models in the next chapter. 

#############confusion matrix
cm <- confusionMatrix(as.factor(qda.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#############
sens<- caret::sensitivity(table(as.factor(qda.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(qda.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]
qda<- c(sens , sp, acc)
#############remove temporary element
rm(cm, sens, sp, acc)
#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv", number = 10,
repeats=5, sampling = "down")
mod_qda_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="qda",
family="binomial",
metric = "Sens", #optimize sensitivity
maximize = TRUE, #maximize the metric
trControl= train_params)
qda.pred.b <- predict(mod_qda_fitbalance, newdata = TestData)
#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(qda.pred.b))
names(plot_pred_balance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio QDA - testing balance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_balance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(qda.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#########
sens.b<- caret::sensitivity(table(as.factor(qda.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(qda.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]
qda_balance<- c(sens.b , sp.b, acc.b)
#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)
#Same division
set.seed(1234)
#########################model######################################vvvv
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5) #K-Fold Cross Validation
library(earth)
mod_fda_fit <- caret::train(RESPONSE ~ ., TrainData, method="fda",
trControl= train_params)
################check outputs################################vv
mod_fda_fit
fda.pred <- predict(mod_fda_fit, newdata = TestData)
#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(fda.pred))
names(plot_pred_unbalance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio FDA - testing unbalance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_unbalance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(fda.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#############
sens<- caret::sensitivity(table(as.factor(fda.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(fda.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]
fda<- c(sens , sp, acc)
#############remove temporary element
rm(cm, sens, sp, acc)
#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv", number = 10,
repeats=5, sampling = "down")
mod_fda_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="fda",
metric = "Sens", #optimize sensitivity
maximize = TRUE,
trControl= train_params)
################check outputs################################vv
summary(mod_fda_fitbalance)
fda.pred.b <- predict(mod_fda_fitbalance, newdata = TestData)  #predict give me the probability i am looking for the the binomial answer
#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(fda.pred.b))
names(plot_pred_balance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio FDA - testing balance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_balance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(fda.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#####################
sens.b<- caret::sensitivity(table(as.factor(fda.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(fda.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]
fda_balance<- c(sens.b , sp.b, acc.b)
#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)
#Same division
set.seed(1234)
#########################model######################################vvvv
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5) #K-Fold Cross Validation
mod_mda_fit <- caret::train(RESPONSE ~ ., TrainData, method="mda",
family="binomial",trControl= train_params)
################check outputs################################vv
summary(mod_mda_fit)
mda.pred <- predict(mod_mda_fit, newdata = TestData)
#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(mda.pred))
names(plot_pred_unbalance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio MDA - testing unbalance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_unbalance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(mda.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#############remove temporary element
sens<- caret::sensitivity(table(as.factor(mda.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(mda.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]
mda<- c(sens , sp, acc)
#############remove temporary element
rm(cm, sens, sp, acc)
#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv", number = 10,
repeats=5, sampling = "down")
mod_mda_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="mda",
family="binomial",
metric = "Sens", #optimize sensitivity
maximize = TRUE,
trControl= train_params)
################check outputs################################vv
mod_mda_fitbalance
mda.pred.b <- predict(mod_mda_fitbalance, newdata = TestData)  #predict give me the probability i am looking for the the binomial answer
#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(mda.pred.b))
names(plot_pred_balance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio MDA - testing balance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_balance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(mda.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#########
sens.b<- caret::sensitivity(table(as.factor(mda.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(mda.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]
mda_balance<- c(sens.b , sp.b, acc.b)
#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)
#Same division
set.seed(1234)
#########################model######################################vvvv
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5) #K-Fold Cross Validation
mod_rf_fit <- caret::train(RESPONSE ~ ., TrainData, method="rf",
trControl= train_params)
################check outputs################################vv
print(mod_rf_fit)
#Variable importance analysis
rf.fit <- randomForest::randomForest(RESPONSE ~.,
TrainData,
ntree = 500,
mtry = 4,
importance = TRUE,
replace = FALSE)
randomForest::importance(rf.fit)
randomForest::varImpPlot(rf.fit, main ='Variable Importance')
#two criteria: MeanDecreaseAccuracy (rough estimate of the loss in prediction performance when that particular variable is omitted from the training set) and Mean Decrease Gini (GINI is a measure of node impurity, highest purity means that each node contains only elements of a single class. Assessing the decrease in GINI when that feature is omitted leads to an understanding of how important that feature is to split the data correctly)
rf.pred <- predict(mod_rf_fit, newdata = TestData)  #predict give me the probability i am looking for the the binomial answer
#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(rf.pred))
names(plot_pred_unbalance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio Random Forest - testing unbalance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_unbalance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(rf.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
############
sens<- caret::sensitivity(table(as.factor(rf.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(rf.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]
rf<- c(sens , sp, acc)
#############remove temporary element
rm(cm, sens, sp, acc)
train_params <- caret::trainControl(method = "repeatedcv", number = 10,
repeats=5, sampling = "down")
mod_rf_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="rf",
family="binomial",
metric = "Sens", #optimize sensitivity
maximize = TRUE,
trControl= train_params)
train_params <- caret::trainControl(method = "repeatedcv", number = 10,
repeats=5, sampling = "down")
mod_rf_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="rf",
family="binomial",
metric = "Sens", #optimize sensitivity
maximize = TRUE,
trControl= train_params)
################check outputs################################vv
print(mod_rf_fitbalance)
rf.pred.b <- predict(mod_rf_fitbalance, newdata = TestData)
#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(rf.pred.b))
names(plot_pred_balance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio Random Forest - testing balance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_balance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(rf.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
######################
sens.b<- caret::sensitivity(table(as.factor(rf.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(rf.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]
rf_balance<- c(sens.b , sp.b, acc.b)
#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)
#Same division
set.seed(1234)
#########################model######################################
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5)
mod_nn_fit <- caret::train(RESPONSE ~ ., TrainData, method="nnet",
trControl= train_params)
################check outputs################################
print(mod_nn_fit)
library(nnet)
NeuralNetTools::plotnet(mod_nn_fit$finalModel, y_names = "yes/no")
title("Graphical Representation of our Neural Network")
nn.pred <- predict(mod_nn_fit, newdata = TestData)
#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(nn.pred))
names(plot_pred_unbalance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio NN - testing unbalance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_unbalance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(nn.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
#############
sens<- caret::sensitivity(table(as.factor(nn.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(nn.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]
nn<- c(sens , sp, acc)
#############remove temporary element
rm(cm, sens, sp, acc)
train_params <- caret::trainControl(method = "repeatedcv", number = 10,
repeats=5, sampling = "down")
mod_nn_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="nnet",
family="binomial",
metric = "Sens", #optimize sensitivity
maximize = TRUE,
trControl= train_params)
################check outputs################################
print(mod_nn_fitbalance)
nn.pred.b <- predict(mod_nn_fitbalance, newdata = TestData)
#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(nn.pred.b))
names(plot_pred_balance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio NN - testing balance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_balance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(nn.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
######################
sens.b<- caret::sensitivity(table(as.factor(nn.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(nn.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]
nn_balance<- c(sens.b , sp.b, acc.b)
#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)
######################### transform data ############
data_xgboost <- purrr::map_df(data_scale, function(columna) {
columna %>%
as.factor() %>%
as.numeric %>%
{ . - 1 } })
test_xgboost <- sample_frac(data_xgboost, size = 0.249)
train_xgboost <- setdiff(data_xgboost, test_xgboost)
#Convertir a DMatrix
train_xgb_matrix <-   train_xgboost %>%
dplyr::select(- RESPONSE) %>%
as.matrix() %>%
xgboost::xgb.DMatrix(data = ., label = train_xgboost$RESPONSE)
#Convertir a DMatrix
test_xgb_matrix <-  test_xgboost %>%
dplyr::select(- RESPONSE) %>%
as.matrix() %>%
xgboost::xgb.DMatrix(data = ., label = test_xgboost$RESPONSE)
#Same division
set.seed(1234)
#########################model######################################
train_params <- caret::trainControl(method = "repeatedcv",
number = 10, # with n folds
repeats=5) #K-Fold Cross Validation
mod_xgb_fit <- caret::train(RESPONSE ~ ., TrainData,
method="xgbTree",
trControl= train_params)
train_params <- caret::trainControl(method = "repeatedcv", number = 10,
repeats=5, sampling = "down")
mod_xgb_fitbalance <- caret::train(RESPONSE ~ ., TrainData, method="xgbTree",
metric = "Sens", #optimize sensitivity
maximize = TRUE,
trControl= train_params)
################check outputs################################
print(mod_xgb_fit)
mod_xgb_fit$bestTune
xgb.pred <- predict(mod_xgb_fit, newdata = TestData)
#prediction given the model
plot_pred_unbalance <- as.data.frame(as.vector(xgb.pred))
names(plot_pred_unbalance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_unbalance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio XGB - testing unbalance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_unbalance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(xgb.pred), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
######################
sens<- caret::sensitivity(table(as.factor(xgb.pred), as.factor(TestData$RESPONSE)))
sp<- caret::specificity(table(as.factor(xgb.pred), as.factor(TestData$RESPONSE)))
acc<- cm$overall["Accuracy"]
xgb<- c(sens , sp, acc)
#############remove temporary element
rm(cm, sens, sp, acc)
################check outputs################################
print(mod_xgb_fitbalance)
xgb.pred.b <- predict(mod_xgb_fitbalance, newdata = TestData)
#prediction given the model
plot_pred_balance <- as.data.frame(as.vector(xgb.pred.b))
names(plot_pred_balance)<- c("predictions")
#table for the graph of training and testing
p<- ggplot2::ggplot(data=plot_pred_balance, aes(x= predictions, fill=predictions)) +
geom_bar(aes(y = (..count..)/sum(..count..)))+
scale_y_continuous(labels=scales::percent) +
ylab("Freq")+
theme_bw()+
theme(legend.title = element_blank()) +
theme(plot.title = element_text(face = "bold",  hjust = 0.5)) +
labs(title = "Response Ratio XGB - testing balance",
x = "Response [no=0 /yes=1]", y = "Frequency (%)")
p
rm(plot_pred_balance, p)
#############confusion matrix
cm <- confusionMatrix(as.factor(xgb.pred.b), as.factor(TestData$RESPONSE))
############draw of confusion matrix
draw_confusion_matrix(cm)
######################
sens.b<- caret::sensitivity(table(as.factor(xgb.pred.b), as.factor(TestData$RESPONSE)))
sp.b<- caret::specificity(table(as.factor(xgb.pred.b), as.factor(TestData$RESPONSE)))
acc.b<- cm$overall["Accuracy"]
xgb_balance<- c(sens.b , sp.b, acc.b)
#############remove temporary element
rm(cm, sens.b, sp.b, acc.b)
Details<- c("Sensitivity", "Specificity", "Accuracy")
summary_table<- data.frame(Details, logistic, logistic_balance,
decision_tree, decision_tree_balance,
lda, lda_balance,
qda, qda_balance,
fda, fda_balance,
mda, mda_balance,
rf, rf_balance,
nn, nn_balance,
xgb, xgb_balance)
summary_table<-data.frame(t(summary_table[-1]))
names(summary_table) <- c("Sensitivity", "Specificity", "Accuracy")
kableExtra::kable(summary_table, caption = "Summary table for assess the model")%>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
RF <- confusionMatrix(as.factor(rf.pred.b), as.factor(TestData$RESPONSE))$table[2,1]
NN <- confusionMatrix(as.factor(nn.pred.b), as.factor(TestData$RESPONSE))$table[2,1]
XGB <- confusionMatrix(as.factor(xgb.pred.b), as.factor(TestData$RESPONSE))$table[2,1]
FP <- data.frame(t(data.frame(RF, NN, XGB)))
names(FP) <- c("False Positive")
FP
NN
NN
RF
FP %<>% dplyr::mutate(Model = c("RF", "NN", "XGB"),
FP_Perc = (FP[,1]/nrow(TestData))) %>% dplyr::select("Model", everything())
FP
amount <- data_sel[-val_index,]$AMOUNT
fp.rf <- (ifelse(rf.pred.b == 1 & TestData$RESPONSE == 0, 1, 0))
losses.rf <- sum(fp.rf * amount)
fp.nn <- (ifelse(nn.pred.b == 1 & TestData$RESPONSE == 0, 1, 0))
losses.nn <- sum(fp.nn * amount)
fp.xgb <- (ifelse(xgb.pred.b == 1 & TestData$RESPONSE == 0, 1, 0))
losses.xgb <- sum(fp.xgb * amount)
Losses <- data.frame(losses.rf, losses.nn, losses.xgb)
Losses <- data.frame(t(Losses))
names(Losses) <- "Losses"
Losses %<>% dplyr::mutate(Model = c("RF", "NN", "XGB")) %>% dplyr::select(Model, Losses)
Losses
losses.xgb
losses.rf
sel <- data_sel[-val_index,] #getting the observations unscaled
pos <- sel %>% dplyr::filter(RESPONSE == 1) %>% dplyr::select(AMOUNT) #selecting only the amount of the credits that are granted
Losses %<>% dplyr::mutate(Losses_Perc = Losses / sum(pos))
Losses
cbind(FP, Losses[,-1])
Losses[1,3]
Losses[1,3]
names(data_sel[,-1])
names(data_sel[,-1])
names(data_sel[,-1])
setwd("~/Desktop/-/Uni/Third Semester/Projects in Data Analytics for Decision Making/Project/Projects-DA-DecisionMaking/Project")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
################check outputs################################
print(mod_xgb_fitbalance)
print(mod_xgb_fitbalance)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")

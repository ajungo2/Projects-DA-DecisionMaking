"0","#we change everything to a factor or numerico"
"0","data_xgboost <- map_df(data_sel, function(columna) {"
"0","                  columna %>% "
"0","                  as.factor() %>% "
"0","                  as.numeric %>% "
"0","                  { . - 1 } })"
"0","test_xgboost <- sample_frac(data_xgboost, size = 0.249)"
"0","train_xgboost <- setdiff(data_xgboost, test_xgboost)"
"0","#Convertir a DMatrix"
"0","train_xgb_matrix <-   train_xgboost %>% "
"0","                            dplyr::select(- RESPONSE) %>% "
"0","                            as.matrix() %>% "
"0","                            xgb.DMatrix(data = ., label = train_xgboost$RESPONSE)"
"0","#Convertir a DMatrix"
"0","test_xgb_matrix <-  test_xgboost %>% "
"0","                            dplyr::select(- RESPONSE) %>% "
"0","                            as.matrix() %>% "
"0","                            xgb.DMatrix(data = ., label = test_xgboost$RESPONSE)"
"0","#####################Define the parameters##########################################"
"0","#objective = ""binary:logistic"": we will train a binary classification model ;"
"0","#max.depth = 2: the trees wonâ€™t be deep ;"
"0","#nthread = 2: the number of cpu threads we are going to use;"
"0","#nrounds = there will be two passes on the data, the second one will enhance the model by further reducing the difference between ground truth and prediction."
"0","set.seed(124)"
"0","model_xgboost <- xgboost::xgboost(data = train_xgb_matrix , "
"0","                                    objective = ""binary:logistic"","
"0","                                    nrounds = 20, max.depth = 2, eta = 0.3, "
"0","                                    nthread = 2, subsample = 1)"
"1","[1]	train-error:0.249333"
"1"," "
"1","
"
"1","[2]	train-error:0.228000"
"1"," "
"1","
"
"1","[3]	train-error:0.241333"
"1"," "
"1","
"
"1","[4]	train-error:0.232000"
"1"," "
"1","
"
"1","[5]	train-error:0.224000"
"1"," "
"1","
"
"1","[6]	train-error:0.221333"
"1"," "
"1","
"
"1","[7]	train-error:0.225333"
"1"," "
"1","
"
"1","[8]	train-error:0.220000"
"1"," "
"1","
"
"1","[9]	train-error:0.216000"
"1"," "
"1","
"
"1","[10]	train-error:0.216000"
"1"," "
"1","
"
"1","[11]	train-error:0.197333"
"1"," "
"1","
"
"1","[12]	train-error:0.189333"
"1"," "
"1","
"
"1","[13]	train-error:0.193333"
"1"," "
"1","
"
"1","[14]	train-error:0.192000"
"1"," "
"1","
"
"1","[15]	train-error:0.198667"
"1"," "
"1","
"
"1","[16]	train-error:0.192000"
"1"," "
"1","
"
"1","[17]	train-error:0.178667"
"1"," "
"1","
"
"1","[18]	train-error:0.186667"
"1"," "
"1","
"
"1","[19]	train-error:0.182667"
"1"," "
"1","
"
"1","[20]	train-error:0.177333"
"1"," "
"1","
"
"0","model_xgboost"
"1","##### xgb.Booster
"
"1","raw: "
"1","10.7 Kb"
"1"," "
"1","
"
"1","call:
  "
"1","xgb.train(params = params, data = dtrain, nrounds = nrounds, 
"
"1","    watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, 
"
"1","    early_stopping_rounds = early_stopping_rounds, maximize = maximize, 
"
"1","    save_period = save_period, save_name = save_name, xgb_model = xgb_model, 
"
"1","    callbacks = callbacks, objective = ""binary:logistic"", max.depth = 2, 
"
"1","    eta = 0.3, nthread = 2, subsample = 1)
"
"1","params (as set within xgb.train):
"
"1","  "
"1","objective = ""binary:logistic"", max_depth = ""2"", eta = ""0.3"", nthread = ""2"", subsample = ""1"", validate_parameters = ""TRUE"""
"1","
"
"1","xgb.attributes:
"
"1","  "
"1","niter"
"1","
"
"1","callbacks:
"
"1","  "
"1","cb.print.evaluation(period = print_every_n)
"
"1","  "
"1","cb.evaluation.log()
"
"1","# of features:"
"1"," "
"1","21"
"1"," "
"1","
"
"1","niter: "
"1","20"
"1","
"
"1","nfeatures"
"1"," "
"1",":"
"1"," "
"1","21"
"1"," "
"1","
"
"1","evaluation_log:
"
"1","   "
"1"," iter"
"1"," train_error"
"1","
   "
"1","    1"
"1","       0.249"
"1","
   "
"1","    2"
"1","       0.228"
"1","
---"
"1","     "
"1","            "
"1","
   "
"1","   19"
"1","       0.183"
"1","
   "
"1","   20"
"1","       0.177"
"1","
"
"0","####################Training & Test Error Plot ####################"
"0","#if they are more close better the accuracy"
"0","e <- data.frame(model_xgboost$evaluation_log)"
"0","plot(e$iter, e$train_mlogloss, col = 'blue')"

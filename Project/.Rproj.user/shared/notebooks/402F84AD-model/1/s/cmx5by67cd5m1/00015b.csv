"0",""
"0","#we change everything to a factor or numerico"
"0",""
"0","data_xgboost <- map_df(data_sel, function(columna) {"
"0","                  columna %>% "
"0","                  as.factor() %>% "
"0","                  as.numeric %>% "
"0","                  { . - 1 } })"
"0",""
"0","test_xgboost <- sample_frac(data_xgboost, size = 0.249)"
"0","train_xgboost <- setdiff(data_xgboost, test_xgboost)"
"0",""
"0",""
"0","#Convertir a DMatrix"
"0",""
"0","train_xgb_matrix <-   train_xgboost %>% "
"0","                            dplyr::select(- RESPONSE) %>% "
"0","                            as.matrix() %>% "
"0","                            xgb.DMatrix(data = ., label = train_xgboost$RESPONSE)"
"0","#Convertir a DMatrix"
"0",""
"0","test_xgb_matrix <-  test_xgboost %>% "
"0","                            dplyr::select(- RESPONSE) %>% "
"0","                            as.matrix() %>% "
"0","                            xgb.DMatrix(data = ., label = test_xgboost$RESPONSE)"
"0",""
"0","#####################Define the parameters##########################################"
"0",""
"0","#objective = ""binary:logistic"": we will train a binary classification model ;"
"0","#max.depth = 2: the trees wonâ€™t be deep ;"
"0","#nthread = 2: the number of cpu threads we are going to use;"
"0","#nrounds = 2: there will be two passes on the data, the second one will enhance the model by further reducing the difference between ground truth and prediction."
"0",""
"0","set.seed(124)"
"0","model_xgboost <- xgboost::xgboost(data = train_xgb_matrix , "
"0","                                    objective = ""binary:logistic"","
"0","                                    nrounds = 20, max.depth = 2, eta = 0.3, "
"0","                                    nthread = 2, subsample = 1)"
"1","[1]	train-error:0.302667"
"1"," "
"1","
"
"1","[2]	train-error:0.281333"
"1"," "
"1","
"
"1","[3]	train-error:0.276000"
"1"," "
"1","
"
"1","[4]	train-error:0.261333"
"1"," "
"1","
"
"1","[5]	train-error:0.266667"
"1"," "
"1","
"
"1","[6]	train-error:0.254667"
"1"," "
"1","
"
"1","[7]	train-error:0.238667"
"1"," "
"1","
"
"1","[8]	train-error:0.236000"
"1"," "
"1","
"
"1","[9]	train-error:0.224000"
"1"," "
"1","
"
"1","[10]	train-error:0.224000"
"1"," "
"1","
"
"1","[11]	train-error:0.210667"
"1"," "
"1","
"
"1","[12]	train-error:0.217333"
"1"," "
"1","
"
"1","[13]	train-error:0.214667"
"1"," "
"1","
"
"1","[14]	train-error:0.210667"
"1"," "
"1","
"
"1","[15]	train-error:0.209333"
"1"," "
"1","
"
"1","[16]	train-error:0.205333"
"1"," "
"1","
"
"1","[17]	train-error:0.196000"
"1"," "
"1","
"
"1","[18]	train-error:0.190667"
"1"," "
"1","
"
"1","[19]	train-error:0.186667"
"1"," "
"1","
"
"1","[20]	train-error:0.180000"
"1"," "
"1","
"
"0","model_xgboost"
"1","##### xgb.Booster
"
"1","raw: "
"1","10.7 Kb"
"1"," "
"1","
"
"1","call:
  "
"1","xgb.train(params = params, data = dtrain, nrounds = nrounds, 
"
"1","    watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, 
"
"1","    early_stopping_rounds = early_stopping_rounds, maximize = maximize, 
"
"1","    save_period = save_period, save_name = save_name, xgb_model = xgb_model, 
"
"1","    callbacks = callbacks, objective = ""binary:logistic"", 
"
"1","    max.depth = 2, eta = 0.3, nthread = 2, subsample = 1)
"
"1","params (as set within xgb.train):
"
"1","  "
"1",""
"1","objective = ""binary:logistic"", max_depth = ""2"", eta = ""0.3"", nthread = ""2"", subsample = ""1"", validate_parameters = ""TRUE"""
"1",""
"1","
"
"1","xgb.attributes:
"
"1","  "
"1",""
"1","niter"
"1",""
"1","
"
"1","callbacks:
"
"1","  "
"1","cb.print.evaluation(period = print_every_n)
"
"1","  "
"1","cb.evaluation.log()
"
"1","# of features:"
"1"," "
"1","21"
"1"," "
"1","
"
"1","niter: "
"1",""
"1","20"
"1",""
"1","
"
"1","nfeatures"
"1"," "
"1",":"
"1"," "
"1","21"
"1"," "
"1","
"
"1","evaluation_log:
"

{
    "collab_server" : "",
    "contents" : "# Evaluation\n\n\n## Evaluation of results\nIn this chapter we will assesses the degree to which the model we have chosen meets the business objectives and we will try to determine if there is some business reason why this model is deficient. \n\nThe process will be to compare the results with the evaluation criteria we determined in chapter 3, business understanding. \n\nThe business goal of this analysis was to determine whether a client was at risk of not being able to pay back the credit that has been granted to them, as it would mean a loss for the company and the shareholders.\n\nWe will determine it by considering that the copmany will grant a credit only to those who a have a good credite score, which is those who have the response variable positive, and not giving it to those who have a response of zero.\n\nIn order to do it, we will have a look at the quantity of false positive that the model generated, as they would be the people that would not be able to pay back the company, and we will try to make an evaluation of the potential losses that the company could make in using the specific model, they should be lower than the 10% of the total amount of credits that the company would be willing to accept.\n\nThe one we will look at in the specific are the balanced versions of the neural network, random forest and the xgboot, as they were the ones having the highest performance in terms of all the parameters we are looking at: specificity, sensitivity, accuracy. \n\n```{r false positive}\n\nRF <- confusionMatrix(as.factor(rf.pred.b), as.factor(TestData$RESPONSE))$table[2,1]\n\nNN <- confusionMatrix(as.factor(nn.pred.b), as.factor(TestData$RESPONSE))$table[2,1]\n\nXGB <- confusionMatrix(as.factor(xgb.pred.b), as.factor(TestData$RESPONSE))$table[2,1]\n\nFP <- data.frame(t(data.frame(RF, NN, XGB)))\nnames(FP) <- c(\"False Positive\")\nFP\n\n```\n\nThe table shows the number of false positive instances in the predictions given by each models. As we can see, the lowest value belongs to the neural network, and it's equal to `r NN`. This means that at least in 14 cases, the model would falsly predict a person belonging to the category that should have a credit granted, while it should not. These cases are risky for the company, as they could result in a default in the payback of the credit and hence in a loss of the company. \n\nHowever, the models are still quite satisfying, as the false positive are only a low percentage compared to the number of observations that are tested, you can find the values in the following tables.\n\n```{r percentage}\n\nFP %<>% dplyr::mutate(Model = c(\"RF\", \"NN\", \"XGB\"), \n                     FP_Perc = (FP[,1]/nrow(TestData))) %>% dplyr::select(\"Model\", everything())\nFP\n```\n\nWe can see that the 3 models we have chosen have a percentage of false positive that is lower than 10%. However, the test set is quite small, hence we should repeat the testing with more data to make sure that the values are kept this low. \n\nWe can calculate the maximum losses that could happen if all the people that belongs to the false positive group will not actually pay back the credit they have been granted.\n\n```{r losses}\n\namount <- data_sel[-val_index,]$AMOUNT\n\nfp.rf <- (ifelse(rf.pred.b == 1 & TestData$RESPONSE == 0, 1, 0))\nlosses.rf <- sum(fp.rf * amount)\n\nfp.nn <- (ifelse(nn.pred.b == 1 & TestData$RESPONSE == 0, 1, 0))\nlosses.nn <- sum(fp.nn * amount)\n\nfp.xgb <- (ifelse(xgb.pred.b == 1 & TestData$RESPONSE == 0, 1, 0))\nlosses.xgb <- sum(fp.xgb * amount)\n\nLosses <- data.frame(losses.rf, losses.nn, losses.xgb)\nLosses <- data.frame(t(Losses))\nnames(Losses) <- \"Losses\"\nLosses %<>% dplyr::mutate(Model = c(\"RF\", \"NN\", \"XGB\")) %>% dplyr::select(Model, Losses)\nLosses\n\n```\n\nAs we can see, the amounts ranges from `r losses.nn` to `r losses.rf`. What is surprising is the fact that the random forest model performs better in terms of predicting the false positives (the percentage is lower compared to the one of the xgb for example), however, it has a higher value for the losses. This means that the XGB probably puts a higher importance on the variable of amount to predict the category of a new person, and tries to minimize the losses as much as possible. It should hence be preferred to the random forest. \n\nWe want to determine whether these losses represent a high percentage of the total amount of credit that would be granted to the people belonging to the test set. \n\n```{r losses percentage}\n\nsel <- data_sel[-val_index,] #getting the observations unscaled \npos <- sel %>% dplyr::filter(RESPONSE == 1) %>% dplyr::select(AMOUNT) #selecting only the amount of the credits that are granted\n\nLosses %<>% dplyr::mutate(Losses_Perc = Losses / sum(pos))\nLosses\n\n```\n\nAs we can see, the model that as the lowest percentage is the neural network and it does meet our criteria for the selection of the model. i.e.: having the losses lower than the 10% of the total amount of the credits that would be granted. \n\nHowever, we can also say that the percentage of the losses given by the random forest and the XGB are exceeding the threshold by less than 2%, hence it could be discussed to also use one of these models if it would mean a lower cost for the company in terms of complexity and computation time. \nThis applies more for the random forest than for the XGB, as we could see that the latter was taking quite some time to be fitted. What is more, the random forest allows for a higher degree of interpretation, while the neural network is more used as a *black box*. \n\n```{r evaluation}\n\ncbind(FP, Losses[,-1])\n\n```\n\nWe would hence suggest to use a **random forest model**, as it has among the highest sensitivity, lowest amount of false positive predictions and a percentage of losses that is equal to `r Losses[1,3]`, while having also a higher degree of interpretability and lower complexity, compared to the other methods that were selected at the end of our modelization chapter. \n\nMoreover, we have seen that not all the variables that are included in the dataset are actually useful for the prediction of the response. This means that the copmany, when evaluating a new customer, should rather focus on getting the information regarding the variables that have been selected, namely `r names(data_sel[,-1])`. This would mean lower costs for the company, as they would spend less time on getting useless information and less space to store them. \n\n## Review the process \n\n### Overview \n\nWe started our data mining with an exploratory data analysis.\nWe looked at the structure of the dataset that we used, which had 32 variables and 1'000 observations. Then, we had a more detailed look at the output variable and we could conclude that we had a binary with a majority of positive instances. Looking at the independent variables, we could see that the continuous ones were skewed and had different scales. We could also identify some errors in the data that were fixed, while no missing values were founded. In the second part of our EDA we built a few category variables, so that we could diminish the number of variables that we needed to use in the modelling, more specifically we built a binary describing the sex of the person, one categorical for the purpose of the credit, one categorical for the property and another one for the residence, to assess if it made sense to aggregate the variables, some chi-squared test were run. To further select the data, we created a simple linear regression and we used the AIC to select only the most significant ones. \nWe were then able to move on to the modelling part, in which we used 6 different models, namely: logistic regression, decision trees, discriminate analysis, random forest, neural network and xgboost. For each of them we fit a model on the unbalanced training set (containing 75% of the data randomly selected) and then compared the predictions it gave to the test set (conatining the remaining 25% of the data). We did also a balancing of the dataset, in order to have around the same amount of the positive and negative values for the response, and we fit the same models on the training set based on this data, built in the same way as before, and compared the predictions to the test set. \n\n### Improvements \n\nWe believe that what has been done was an accurate analysis of the data, however some improvements could be done in terms of process performance.\nMore specifically, we could see that the variable created describing the sex of the person was not selected, hence it was not necessary to create it. Moreover, the correlation were calculated but not really used for the selection of the variables, they could have been avoided too. \nWe also believe that the coding could have been executed in a more efficient way, as a lot of repetitions were done, specifically in the modelling part. We could have created either a function for the modelling and use is to diminish the lines of code, or find another way to optimize it, e.g. the use of a different library. However, thanks to the `caret` package, we were already able to optimize a good part of the code, which would have been even longer and more complicated otherwise. \nWhat is more, we could have included different models, as some of the ones we used are elementary and were expected to perform poorly compared to more complex ones such as the neural network or the random forest. We could have chosen *one* simple model in order to compare the results and see if the increase in accuracy, sensitivy and specificity was high enough and then only keep the most performing ones and select some others. \n\nIn any case, the results we have found are quite satisfying, as we could still find three models that are giving a prediction that is meeting (or almost) our business success criteria. \n\n## Next Steps\n\nTo improve the process, another model could be selected, maybe one that has not been considered in our analysis. However, we beleve that the results that will be given are already satisfying enough. \n\nAnother way to improve the model could be to considered other information that has not been considered in our analysis, such as the number of other credits that are pending or the history of (un)repaid credits. \n\nAn alternative way could be to gather other information from other credit companies, banks, insurances, etc., so that it is possible to fit a more powerful model. \n\n### Decision \n\nWith our analysis, the company should be able to assess the quality of a new customer and predict if it should be a good idea to give them a credit or not. \nWe believe that the company should follow these steps, each time a new customer approaches the firm from now on:\n1. Collect the information only regarding the variables that have been selected, namely `r names(data_sel[,-1])`\n2. With the information gathered, run a random forest model prediction and determine whether the credit should be granted or not\n3. Store the result of the decision\n4. In case the credit was given, wait and see if it will be paid back \n5. Store the result of the debt settlement \n6. Use the new data to fit an upgraded model \n\n\n",
    "created" : 1606982581386.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1811781301",
    "id" : "DBD6E66E",
    "lastKnownWriteTime" : 1607000908,
    "last_content_update" : 1607000908890,
    "path" : "~/Desktop/-/Uni/Third Semester/Projects in Data Analytics for Decision Making/Project/Projects-DA-DecisionMaking/Project/report/evaluation.Rmd",
    "project_path" : "report/evaluation.Rmd",
    "properties" : {
    },
    "relative_order" : 8,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}
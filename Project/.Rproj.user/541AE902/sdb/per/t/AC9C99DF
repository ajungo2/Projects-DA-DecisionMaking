{
    "collab_server" : "",
    "contents" : "# Model \n \n```{r confusion matrix draw function, include=FALSE}\n\ndraw_confusion_matrix <- function(cm) {\n\n  layout(matrix(c(1,1,2)))\n  par(mar=c(2,2,2,2))\n  plot(c(100, 345), c(300, 450), type = \"n\", xlab=\"\", ylab=\"\", xaxt='n', yaxt='n')\n  title('CONFUSION MATRIX', cex.main=2)\n\n  # create the matrix \n  rect(150, 430, 240, 370, col='#3F97D0')\n  text(195, 435, '0', cex=1.2)\n  rect(250, 430, 340, 370, col='#F7AD50')\n  text(295, 435, '1', cex=1.2)\n  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)\n  text(245, 450, 'Actual', cex=1.3, font=2)\n  rect(150, 305, 240, 365, col='#F7AD50')\n  rect(250, 305, 340, 365, col='#3F97D0')\n  text(140, 400, '0', cex=1.2, srt=90)\n  text(140, 335, '1', cex=1.2, srt=90)\n\n  # add in the cm results \n  res <- as.numeric(cm$table)\n  text(195, 400, res[1], cex=1.6, font=2, col='white')\n  text(195, 335, res[2], cex=1.6, font=2, col='white')\n  text(295, 400, res[3], cex=1.6, font=2, col='white')\n  text(295, 335, res[4], cex=1.6, font=2, col='white')\n\n  # add in the specifics \n  plot(c(100, 0), c(100, 0), type = \"n\", xlab=\"\", ylab=\"\", \n       main = \"DETAILS\", xaxt='n', yaxt='n')\n  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)\n  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)\n  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)\n  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)\n  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)\n  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)\n  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)\n  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)\n  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)\n  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)\n\n  # add in the accuracy information \n  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)\n  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)\n  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)\n  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)\n}  \n\n```\n\n## Selecting modelling technique \n\nThe modelling technique that we will be using are the following:\n\n\n| n°  | Model                      | Function in R |\n|:---:|:---------------------------|:--------------|\n|  1  | Logistic regression models |    GLM        |\n|  2  | Decision trees             |   CART        |\n|  3  | Discriminate analysis      | 1) lda <br /> 2) qda <br /> 3) mda <br /> 4) fda |\n|  4  | Random forest              |  randomForest |\n|  5  | XGBoost                    |    xgboost    |\n\n## Generate test design\n\nFirstly, we need to standardize the data, as the variables have different scales. We will normalize only the continuous variables, as the categorical and dummy variables have only few different levels.  \n\n```{r scaling}\n\n#selecting only the continuous variables to scale them\n\ndata_scale <- data_sel %>% \n                dplyr::select(DURATION, AMOUNT, INSTALL_RATE, NUM_CREDITS) %>% \n                scale() %>%  #normalization\n                as.data.frame()\n\n#recreating the other variables to add them back to the dataset of the scaled ones \n\ndata_scale %<>% mutate(\n  RESPONSE = data_sel$RESPONSE,\n  CHK_ACCT = data_sel$CHK_ACCT,\n  HISTORY = data_sel$HISTORY,\n  PURPOSE = data_sel$PURPOSE,\n  SAV_ACCT = data_sel$SAV_ACCT,\n  EMPLOYMENT = data_sel$EMPLOYMENT,\n  INSTALL_RATE = data_sel$INSTALL_RATE,\n  MALE_SINGLE = data_sel$MALE_SINGLE,\n  GUARANTOR = data_sel$GUARANTOR,\n  PROPERTY = data_sel$PROPERTY,\n  OTHER_INSTALL = data_sel$OTHER_INSTALL,\n  RESIDENCE = data_sel$RESIDENCE,\n  TELEPHONE = data_sel$TELEPHONE\n)\n\n#reordering variable in the dataset\n\ndata_scale %<>% \n  dplyr::select(RESPONSE, CHK_ACCT, DURATION, HISTORY, PURPOSE, AMOUNT, SAV_ACCT, EMPLOYMENT, INSTALL_RATE, MALE_SINGLE, GUARANTOR, PROPERTY, OTHER_INSTALL, RESIDENCE, NUM_CREDITS, TELEPHONE)\n\n```\n\nWe will then move on by creating a training and test set based on the data.\n\nThis will be done by dividing it in a randomly selection into the two subsets, with 75% of the data in the training set and the remaining 25% in the test set.\n\n```{r test and training sets}\n\n#so that we always have the same division\nset.seed(2311)\n\n#creation of the index to divide the data in the two subsets\nval_index<-createDataPartition(data_scale$RESPONSE, \n                               p=0.75, list=FALSE)\n\n#training dataset\nTrainData<-as.data.frame(data_scale[val_index,])\n\n#test dataset\nTestData <- data_scale[-val_index,]\n\n```\n\nWe can now move to the modelization. Let's start with the first model. \n\n## Build models \n\n### 1) Logistic Regressions\n\n#### GLM\n\n##### Definition\n\nWe will start by giving a definition of this model. \n\n> Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). \n(https://en.wikipedia.org/wiki/Logistic_regression)\n\nThe model of ligistic regression is the following:\n\n\\[ Z_{i} = ln(\\frac{P_{i}} {1-P_{i}}) = \\beta_0+\\beta_1X_1+...+\\beta_nX_n \\]\n\nTo do so, we are going follow 6 steps:\n1) Fit the model\n2) Check for class bias\n3) Create training and test samples\n4) Compute information value to find out important variables\n5) Build logit models and predict on test data\n6) Do model diagnostics\n\n##### Fitting the model \n\nNow we will fit the logistic regression on the training dataset.\n\n```{r glm-model, echo=FALSE, warning=FALSE, message=FALSE}\n\ntable(data_sel$RESPONSE) #--> there is a class bias in the data in general\ntable(TrainData$RESPONSE) #--> there is a class bias in the training\n\n#Same division\nset.seed(1234)\n\n#model\nglm.fit <- glm(RESPONSE ~., data = TrainData, family = binomial(link=\"logit\")) #family=binomial is key for classification\n\n#check outputs\nsummary(glm.fit)\n\n```\n\nHere, we can see that the variables that take the highest importance and that are statistically significant for the model are: the second and third level of `CHK_ACC`, `DURATION`, the fourth level of `HISTORY`,the first level of `PURPOSE`, `AMOUNT`,the fourth level of `SAV_ACCT`, the second level of `EMPLOYMENT`, `OTHER_INSTALL` (being positive) and `TELEPHONE`.\n\n```{r glm coef}\n\nglm.fit$coefficients\n\n```\n\nIf we look at the coeffiecient of the different variables we can conclude that, among the significant one that we described before, `CHK_ACCT`, `HISTORY` (all but the first level), `SAV_ACCT`, `EMPLOYMENT` and `MALE_SINGLE`, have a positive impact on the output, meaning that the higher is their level or if they are positive, the probability of having `RESPONSE` = 1 will increase. \n\nOn the other hand, among the significant variables, `DURATION`, `PURPOSE` (all but level two and four), `AMOUNT` and `OTHER_INSTALL` have a negative effect on the output, meaning that if they increase their level or value, or if they have a positive value (for the dummies), the probability of having a positive response will decrease.\n\nThe linear predictor is given by \n$$ \\eta  = - 0.1311 + 0.2353 * CHKACCT_1 + 1.1136 * CHKACCT_2  + 1.6447 * CHKACCT_3 - 0.3090 * DURATION - 0.3719 * HISTORY_1 + 0.4818 * HISTORY_2 +  0.6720 * HISTORY_3 + 1.3959 * HISTORY_4 - 0.9133 * PURPOSE_1 + 0.8385 * PURPOSE_2 - 0.0585 * PURPOSE_3 + 0.0770 * PURPOSE_4 - 0.7794 * PURPOSE_5 - 0.0920 * PURPOSE_6 - 0.3923 * AMOUNT - 0.3923 * SAVACCT_1 + 0.5117 * SAVACCT_2 + 0.7612 * SAVACCT_3 + 1.3216 * SAVACCT_4 + 0.3373 * EMPLOYMENT_1 + * 0.7197 EMPLOYMENT_2 + 1.1494 * EMPLOYMENT_3 + 0.7664 * EMPLOYMENT_4 - 0.3365 * INSTALLRATE + 0.5638 * MALESINGLE_1 + 0.9074 * GUARANTOR_1 + 0.1066 * PROPERTY_1 - 0.6069 * PROPERTY_2 - 0.5723 * OTHERINSTALL_1 - 0.8100 * RESIDENCE_1 - 0.2967 * RESIDENCE_2 - 0.1347 * NUMCREDITS + 0.4426 * TELEPHONE_1 $$\n\nTo be clear, if for example the purpose variable takes value 3, only the coefficient of PURPOSE_3 will be added to the others. The same goes for each other categorical variable. For the dummies the coefficient is added only if the value is equal to 1 for the variable, otherwise no. While for the continuous variables the coefficient is multiplied by the value is recorded in the observation. \n\n##### Predictions and confusion matrix \n\nNow we will get the predictions using this model. To do it, we will start by getting the probabilities of the output given the coefficients we have found by fitting the model, then we will use a cut point of 0.5 to decide whether the value will be equal to 1 (if the probability it higher than 0.5) or 0 (otherwise).\nThe model basically fit the information of the new observation in the function that is given above, and then it finds a value *eta* that is then used to get the prediction of the probability of the output by doint p = 1 / (1 + eta). We use this probability to determine whether the prediciton will by 1 or 0, by taking a cut point of 0.5. \n\n```{r predictions}\n\n#probability given the model\nglm.probs <- predict(glm.fit, newdata = TestData, type = \"response\")  #predict give me the probability i am looking for the the binomial answer\n\n#use a cut of 0.5 to give the prediction\nglm.pred <- ifelse(glm.probs > 0.5, 1, 0)\n\n```\n\n##### Model diagnosis \n\nWe can move on to the diagnosis part of our modelization process. We start by looking at the multicollinearity of the variables, to check that none of them present this problem. In order to do so, we use the VIF. \n\n```{r multicollinearity check}\n\ncar::vif(glm.fit)\n\n```\n\nWe can see that none of the variable present a value higher than 5, hence the problem of multicollinearity is not present here.\n\n##### Predictions and confusion matrix \n\nThen, with the prediction we can build the confusion matrix. \n\n```{r confusion matrix}\n\n#confusion matrix \ncm <- confusionMatrix(as.factor(glm.pred), as.factor(TestData$RESPONSE))\n#draw of confusion matrix\ndraw_confusion_matrix(cm)\n#remove temporary element\nrm(cm)\n\n```\n\nWe are more interested in the false prediction of negative outcomes, as it would mean a loss for the company, rather than a false prediction of a positive outcome, hence we will look at the specificity rather than the sensitivity. We can see that in 35 cases in which the model should have predicted a negative value, it actually predicted a postive one. Here we can see the the prediction of negative values is quite high (above 90% of the time when an element is predicted as negative it if truly negative) and also if it is negative it will be predicted as such. \n\nWe could believe that this is due to an unbalance of the data set, but as we have seen in the EDA, we have a dataset with a higher presence of positive value than negative, hence it would be easier for the model to predict a positive value, which is not the case. \n\nIn any case, we will repeat the prediction with a balanced training set, in order to avoid any bias. \n\n### Decision Trees\n\n#### CART\n\n##### Definition\n\n> A Classification And Regression Tree (CART), is a predictive model, which explains how an outcome variable's values can be predicted based on other values. A CART output is a decision tree where each fork is a split in a predictor variable and each end node contains a prediction for the outcome variable.\n\n(source: https://wiki.q-researchsoftware.com/wiki/Machine_Learning_-_Classification_And_Regression_Trees_(CART))\n\n##### Fitting the model \n\nWe will start by fitting the model on the data.\n\n```{r cart}\n\n#library(rpart)\n#library(rpart.plot)\nct.fit <- rpart(RESPONSE ~., method = \"class\",  \n                data = TrainData, #the training data\n                control = rpart.control(minsplit = 4, \n                cp = 1e-05) , model = TRUE)\n\nrpart.plot(ct.fit, extra=104, box.palette=\"GnBu\",\n               branch.lty=3, shadow.col=\"gray\", nn=TRUE)\n\n```\n\nWe can see that the tree is quite big, meaning that there may be too many variables used in the model. However, we will move on to the predictions and the confusion matrix to be able to better assess the fit of this model. \n\n<!-- > Pruning is a Data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are uncritical and redundant to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting. -->\n<!-- source:https://en.wikipedia.org/wiki/Decision_tree_pruning -->\n\n\n```{r pruning cart, echo=FALSE}\n\n# #cp graph\n# par(pty = \"s\")\n# plotcp(ct.fit)\n# \n# #cp that minimize the tree\n# bestcp <- ct.fit$cptable[which.min(ct.fit$cptable[,\"xerror\"]),\"CP\"]\n# \n# ct.prune <- prune(ct.fit, cp= bestcp) #before there was cp=0.01\n# \n# summary(ct.prune)\n# \n# #first option of graph:\n# #plot(ct.prune, uniform = TRUE)\n# #text(ct.prune, cex = 0.4, use.n = TRUE, all = TRUE)\n# \n# #second option of graph:\n# rpart.plot(ct.prune, extra=104, box.palette=\"GnBu\",\n#                branch.lty=3, shadow.col=\"gray\", nn=TRUE)\n# \n# \n# #partition tree--> lets check later....\n# set.seed(123) ## For consistent jitter\n# \n# partition_data <- TrainData\n# partition_data$RESPONSE <- as.factor(partition_data$RESPONSE)\n# \n# ## Build our tree using parsnip (but with rpart as the model engine)\n# \n# t_tree <-\n#   decision_tree() %>%\n#   set_engine(\"rpart\") %>%\n#   set_mode(\"classification\") %>%\n#   fit(RESPONSE ~ CHK_ACCT + HISTORY , data = partition_data)\n# \n# #install.packages(\"remotes\")\n# # remotes::install_github(\"grantmcdermott/parttree\") \n# # library(parttree) not working\n# \n# ## Plot the data and model partitions\n# # partition_data %>%\n# #   ggplot(aes(x=CHK_ACCT, y=HISTORY)) + #axis to check\n# #   geom_jitter(aes(col=RESPONSE), alpha=0.7) +\n# #   geom_parttree(data = t_tree, aes(fill=RESPONSE), alpha = 0.1) +\n# #   theme_minimal()+\n# #   theme_bw()\n\n```\n\n<!-- The pruned tree has a number of split equal to 5 and it consider the variables CHK_ACCT, HISTORY, PURPOSE, AMOUNT, NUM_CREDITS, PRESENT_RESIDENT.  -->\n\n<!-- The graph that we have creted shows us how a new observation would be classified: -->\n<!-- 1. If the CHK_ACCT is  equal to 0 or 1 go to the next node, otherwise the RESPONSE will be 1; -->\n<!-- 2. It the HISTORY is equal to 0, 1 or 2 go the next node, otherwise REPSONSE will be 1; -->\n<!-- 3. If PURPOSE is equal to 1 or 5 the RESPONSE will be 0, otherwise go the next node; -->\n<!-- 4. If AMOUNT is lower than 1.7, the RESPONSE will be 0, otherwise go the next node; -->\n<!-- 5. If DURATION is higher than 1.9 RESPONSE will 0, otherwise RESPONSE will be 1.  -->\n\n##### Predictions and confusion matrix \n\nWe will now use the model to predict the values for the output variable and then build the confusion matrix. \n\n```{r prediction cart}\n\nct.pred <- predict(ct.fit, newdata = TestData, type = \"class\")\n\n```\n\n\n```{r confusion matric cart}\n\n#confusion matrix of the basic model\ncm <- confusionMatrix(ct.pred, TestData$RESPONSE)\n#drawing cm for basic model\ndraw_confusion_matrix(cm)\n#removing temporary element\nrm(cm)\n\n```\n\nWe can see that here the sensitivity is really low, while the specificity is higher, reaching a value above 78%, which is in any case the one in which we are the most interested. The accuracy is around 70%. Here, in 36 cases in which the model should have given a negative value, it actually predicted a positive one, and it could cost quite a lot to the company. \n\n### Discriminate analysis \n\n#### LDA \n\n##### Definition\n\nWe look now at the LDA, which is defined as:\n\n> The Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.\n(source: http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/)\n\n##### Fitting the model\n\nWe start by fitting the model, we need to create a reference for each observation to use this model, then we can fit it.\n\n```{r lda model}\n\n#we need to create a reference for each observation to use this model \nOBS <- 1:750\n\n#model creation\nlda.fit <- lda(RESPONSE ~., data = TrainData, subset = OBS)\n\nlda.fit\n\n#A Stacked Histogram of the LDA Values \n#The two groups are the groups for response classes.\nplot(lda.fit)\n\n```\n\nWe can see from the graph that the model seems to be better at predictign the positive value as the mean seems to be around the value 1, while for the negative output the mean seems to be a bit lower than 0, which is the value it should take, and it is around -1.\n\nThe linear combination of predictor variables that are used to form the decision rule is the following: \n$$ RESPONSE = -0.3265 * DURATION -0.2747 * HISTORY_1 + 0.8792 * HISTORY_2 + 1.1810 * HISTORY_3 + 1.6214 * HISTORY_4 - 0.7437 * PURPOSE_1 + 0.7736 * PURPOSE_2 + 0.0172 * PURPOSE_3 + 0.2035 * PURPOSE_4 - 0.6116 * PURPOSE_5 + 0.1298 * PURPOSE_6 - 0.2579 * AMOUNT + 0.5066 * SAV_ACCT_1 + 0.7517 * SAV_ACCT_2 + 0.7778 * SAV_ACCT_3 + 1.0997 * SAV_ACCT_4 + 0.6175 * EMPLOYMENT_1 + 1.1982 * EMPLOYMENT_2 + 1.4580 * EMPLOYMENT_3 + 1.1806 * EMPLOYMENT_4 - 0.2897 * INSTALL_RATE - 0.5663 * SEX_MALE_1 + 0.9272 * MALE_SINGLE_1 + 0.5183 * MALE_MAR_WID_1 - 0.0863 * CO_APPLICANT_1 + 0.5084 * GUARANTOR_1 - 0.2437 * PRESENT_RESIDENT_-1 - 0.1913 * PRESENT_RESIDENT_0 - 0.0477 * PRESENT_RESIDENT_1 + 0.0367 * PROPERTY_1 - 0.6374 * PROPERTY_2 + 0.0502 * AGE - 0.4501 * OTHER_INSTALL_1 - 0.7509 * RESIDENCE_1 - 0.2185 * RESIDENCE_2 - 0.0703 * NUM_CREDITS - 1.0570 * JOB_1 - 1.0581 * JOB_2 - 0.8362 * JOB_3$$\n\nEach new observation will be evaluated thanks to this formula, with its information put inside of it. It follows the same principle described for the generalized linear model. \n\n##### Predictions and confusion matrix \n\nUsing the formula above we can get the predictions and subsequently construct the confusion matrix.\n\n```{r lda predictions}\n\nlda.pred <- predict(lda.fit, newdata = TestData)\nlda.class <- lda.pred$class\n\n\n#plot the error of the predictions\n\npar(mfrow=c(1,1))\nplot(lda.pred$x[,1], lda.pred$class, col=TestData$RESPONSE)\n#key takeaway\n#The Predicted Group-1 and Group-2 has been colored (red and green). The mix of red and green color in each group shows the incorrect classification prediction.\n\n```\n\nIn the graph, the red dots corresponds to the incorrect classification preditions. We can see that the majority are in the positive values, which is quite bad for the company, as it would mean granting a credit to someone who is actually at high risk. \n\n```{r confusion matrix lda}\n\n#building the confusion matrix for the lda \ncm <- confusionMatrix(as.factor(lda.class), as.factor(TestData$RESPONSE))\n#drawinf the cf \ndraw_confusion_matrix(cm)\n#removing the temporary element\nrm(cm)\n\n```\n \nHere the sensitivity is higher with respect to the previous models (above 40%), but it is still quite low. If we look at the preicision, is quite low, as it is only around 70%. What is important to note is that 36 times in which the model would have predicted a positive value for the output, it should have been negative, which is something that could cost quite a lot to the copmany. \n \n#### QDA \n\n##### Definition\n\nWe now look at the Quadratic Discriminant Analysis, which is defined in the following way:\n\n> Quadratic Discriminant Analysis is little bit more flexible than LDA, in the sense that it does not assumes the equality of variance/covariance. In other words, for QDA the covariance matrix can be different for each class.\n\n(source: http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/)\n\n##### Fitting the model \n\nWe start by fitting the model.\n\n```{r fit qda1}\n\n#fitting the qda model\nqda.fit <- qda(RESPONSE ~., data = TrainData, subset = OBS)\n#get the result\nqda.fit\n\n```\n\nHowever, as our sample is quite small, we predict that the LDA will be a better fit compared to the QDA. \n\n##### Predictions and confusion matrix \n\nThen we move to the predictions.\n\n```{r qda predictions}\n\n#getting the overall predictions\nqda.pred <- predict(qda.fit, newdata = TestData)\n#extracting only the prediciton values\nqda.class <- qda.pred$class\n\n#plotting predictions against real data\npar(mfrow=c(1,1))\nplot(qda.pred$posterior[,2], qda.pred$class, col=TestData$RESPONSE)\n\n```\n\nHere, it seems still that the majority of the false prediction are in the positive level, however they seem less than before.\n\n```{r confusion matrix qda}\n\n#building the confusion matrix for the QDA\ncm <- confusionMatrix(as.factor(qda.class), as.factor(TestData$RESPONSE))\n#drwaing the matrix\ndraw_confusion_matrix(cm)\n#removing the temporary element\nrm(cm)\n\n``` \n\nAs we predicted, the model performs a little bit worse than the LDA, but for the sensitivity, which is the highest up to now (over 50%), is still quite low, though. The specificity is moderately high (above 85%) as it is the accuracy (above 70%). As we want to have a value for the false positive low, the 35 here is still quite high.\n\n#### MDA \n\n##### Definition\n\nThe third model of discriminant analysis is the MDA:\n\n> For Mixture Discriminant Analysis, there are classes, and each class is assumed to be a Gaussian mixture of subclasses, where each data point has a probability of belonging to each class. Equality of covariance matrix, among classes, is still assumed.\n\n(source: http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/)\n\n##### Fitting the model \n\nWe can fit the model and look at what it gives as result.\n\n```{r fit mda}\n\n#model fitting\nmda.fit <- mda(RESPONSE ~., data = TrainData, subset = OBS)\n#model summary\n(mda.fit)\n\n```\n\nThe summary gives the percentage of the variance that there is inside the different groups that has been created (which in this case are 5), and we can see that the vast majority of the variance is explained thanks to the first three groups (reaching more than 90%), but we could also be satisfied only considering the first two groups (as they each almost 80% of the variance explained).\n\n##### Predictions and confusion matrix \n\nUsing the model we get the prediction for the `RESPONSE` variable and we can construct the confidence matrix for this case.\n\n```{r mda predictions}\n\nmda.pred <- predict(mda.fit, newdata = TestData)\n\n```\n\n```{r confusion matrix mda}\n\n#confusion matrix builiding\ncm <- confusionMatrix(as.factor(mda.pred), as.factor(TestData$RESPONSE))\n#cm draw\ndraw_confusion_matrix(cm)\n#remove temporary element\nrm(cm)\n\n```  \n\nThe sensitivity in this case is around 45%, while the specificity is higher, reaching almost 88%. The accurary is around 75%. The number of false positive is the highest one up to now, being 40, which is bad for the company.\n\n#### FDA \n\n##### Definition\n\nThe last model of discriminant analysis is the FDA, which is by definition:\n\n> Flexible Discriminant Analysis is a flexible extension of LDA that uses non-linear combinations of predictors such as splines. FDA is useful to model multivariate non-normality or non-linear relationships among variables within each group, allowing for a more accurate classification.\n\n(source: http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/)\n\n##### Fitting the model \n\nWe can now fit the model and get the summary of the results.\n\n```{r fit qda}\n\nfda.fit <- fda(RESPONSE ~., data = TrainData, subset = OBS)\nfda.fit\n\n```\n\nHere we see that this model has only one dimensino that, obviously, explains the 100% of the between group variance. We expect that it will perfom poorly in the predicitons.\n\n##### Predictions and confusion matrix \n\nUsing the model we calculate the predicitons for the output variable and then the confusion matrix based on them.\n\n```{r lda predictions1}\n\nfda.pred <- predict(fda.fit, newdata = TestData)\n\n```\n\n```{r confusion matrix lda1}\n\n#bulding cm\ncm <- confusionMatrix(as.factor(fda.pred), as.factor(TestData$RESPONSE))\n#drawing cm\ndraw_confusion_matrix(cm)\n#remove cm element\nrm(cm)\n\n```  \n\nContrary to what it is expected, this model has a sensitivity of almost 50%, among one of the highest up to now, and the specificity is higher than 90%. The accuracy is around 78%. The false positive observations are 36.\n\n### Ensemble Methods\n\n#### Random Forest\n\n##### Definition\n\nWe now move one and we look at the random forest model, which is defined in the following way:\n\n> Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees. \n\n(source: https://en.wikipedia.org/wiki/Random_forest)\n\n##### Fitting the model\n\nWe can start by fitting the model and look at the results it gives.\n\n```{r random forest fit}\n\n#fitting random forest\nrf.fit <- randomForest(RESPONSE ~., \n                       TrainData, \n                       ntree = 500, \n                       mtry = 4, \n                       importance = TRUE, \n                       replace = FALSE)\n\n```\n\n\n```{r results rf, fig.asp=1.5}\n\nprint(rf.fit)\n\n#Variable importance analysis\n\nimportance(rf.fit) \nvarImpPlot(rf.fit, main ='xxxx')\n\n#two criteria: MeanDecreaseAccuracy (rough estimate of the loss in prediction performance when that particular variable is omitted from the training set) and Mean Decrease Gini (GINI is a measure of node impurity, highest purity means that each node contains only elements of a single class. Assessing the decrease in GINI when that feature is omitted leads to an understanding of how important that feature is to split the data correctly)\n\n```\n\nThe summary of the models gives the decrease in accuracy and the decrease of the gini index for each variable in the model, along with the number of trees that are built (500 in our case), the number of variabes that are randomly chones to be tried at each split before choosing which one is the best one to describe the node. Moreover, we can already find the confusion matrix (we will show it better again afterwards to keep the coherence of the analysis throuhgout all the models), with the class errorand the Out-Of-Bag estimate of the error rate. \n\nLet's give some definitions to be clearer:\n\n> Variable importance is the mean decrease of accuracy over all out-of-bag cross validated predictions, when a given variable is permuted after training, but before prediction.\n\n> GINI importance measures the average gain of purity by splits of a given variable. If the variable is useful, it tends to split mixed labeled nodes into pure single class nodes. Splitting by a permuted variables tend neither to increase nor decrease node purities. \n\nsource:https://stats.stackexchange.com/questions/197827/how-to-interpret-mean-decrease-in-accuracy-and-mean-decrease-gini-in-random-fore\n\n> Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging) to sub-sample data samples used for training. OOB is the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample.\nsource: https://en.wikipedia.org/wiki/Out-of-bag_error\n\n##### Predictions and confusion matrix \n\nWe can now use the model to get the predictions for the output variable and build the confusion matrix.\n\n```{r rf prediction}\n\n#obtain probability\nrf.pred <- predict(rf.fit, newdata = TestData, type = \"class\")\n\n```\n\n\n```{r rf confusion matrix}\n\n#build cm for rf \ncm <- confusionMatrix(as.factor(rf.pred), as.factor(TestData$RESPONSE))\n#draw the cm\ndraw_confusion_matrix(cm)\n#remove cm element\nrm(cm)\n\n```\n\nHere the sensitivity is around 46%, the specificity is high (more than 90%) and the accuracy is aroun 77%, while the number of false positive is the highest, having 41 observations. \n\n### XGBoost \n\n#### Model name\n\n##### Definition\n\n##### Fitting the model \n\n\n```{r fit xgboost}\n\n#we change everything to a factor or numerico\n\ndata_xgboost <- map_df(data_sel, function(columna) {\n                  columna %>% \n                  as.factor() %>% \n                  as.numeric %>% \n                  { . - 1 } })\n\ntest_xgboost <- sample_frac(data_xgboost, size = 0.249)\ntrain_xgboost <- setdiff(data_xgboost, test_xgboost)\n\n\n#Convertir a DMatrix\n\ntrain_xgb_matrix <-   train_xgboost %>% \n                            dplyr::select(- RESPONSE) %>% \n                            as.matrix() %>% \n                            xgb.DMatrix(data = ., label = train_xgboost$RESPONSE)\n#Convertir a DMatrix\n\ntest_xgb_matrix <-  test_xgboost %>% \n                            dplyr::select(- RESPONSE) %>% \n                            as.matrix() %>% \n                            xgb.DMatrix(data = ., label = test_xgboost$RESPONSE)\n\n#####################Define the parameters##########################################\n\n#objective = \"binary:logistic\": we will train a binary classification model ;\n#max.depth = 2: the trees won’t be deep ;\n#nthread = 2: the number of cpu threads we are going to use;\n#nrounds = there will be two passes on the data, the second one will enhance the model by further reducing the difference between ground truth and prediction.\n\nset.seed(124)\nmodel_xgboost <- xgboost::xgboost(data = train_xgb_matrix , \n                                    objective = \"binary:logistic\",\n                                    nrounds = 20, max.depth = 2, eta = 0.3, \n                                    nthread = 2, subsample = 1)\n\n\nmodel_xgboost\n\n\n####################Training & Test Error Plot ####################\n#if they are more close better the accuracy\n\ne <- data.frame(model_xgboost$evaluation_log)\nplot(e$iter, e$train_mlogloss, col = 'blue')\nlines(e$iter, e$test_mlogloss, col = 'red')\n\n##################### Important features###############\n\nimp_feature <- xgb.importance(colnames(train_xgb_matrix), model = model_xgboost)\nprint(imp_feature)\nxgb.plot.importance(imp_feature)\n\n#####################Predictions with the model ##########################################\n\npredict_xgb1 <- predict(model_xgboost, test_xgb_matrix)\n#predict_xgb1 <- as.data.frame(predict_xgb1) #---> if we want to create a kagle\nxgboost.pred <- ifelse(predict_xgb1 > 0.5, 1, 0) #change the probability to binory output\n\n####################Confusion Matrix#################\n\n#bulding cm\ncm <- caret::confusionMatrix(as.factor(xgboost.pred), as.factor(TestData$RESPONSE))\n#################draw of confusion matrix\ndraw_confusion_matrix(cm)\n##############vv#remove temporary element\nrm(cm)\n\n\n```\n\n\n\n##### Hyper-parameter Tuning\n\n```{r xgboost tuning}\n\n###############################################\n\nmodel_xgboost_balance <- xgboost::xgboost(data = train_xgb_matrix ,\n                                          objective = \"binary:logistic\",\n                                          nrounds = 20, max.depth = 2, eta = 0.3, nthread = 2,\n                                          subsample = 0.5 ) ###balance\n                                    \nmodel_xgboost_balance\n\n```\n\n\n##### Predictions \n\n```{r xgboost predictions}\n\n#####################Predictions with the model ##########################################\n\npredict_xgb1_balance <- predict(model_xgboost_balance, test_xgb_matrix)\n#predict_xgb1 <- as.data.frame(predict_xgb1) #---> if we want to create a kagle\nxgboost.pred_balance <- ifelse(predict_xgb1_balance > 0.5, 1, 0) #change the probability to binary output\n\n```\n\n\n```{r confusion matrix xgboost}\n####################Confusion Matrix#################\n\n#bulding cm\ncm <- caret::confusionMatrix(as.factor(xgboost.pred_balance), as.factor(TestData$RESPONSE))\n#################draw of confusion matrix\ndraw_confusion_matrix(cm)\n##############vv#remove temporary element\nrm(cm)\n\n\n```  \n\n### Neural Network \n\n#### Model name\n\n##### Definition\n\n##### Fitting the model \n\n\n```{r fit neural}\n\n#library(neuralnet) \nset.seed(333) \ntrain_params <- trainControl(method = \"repeatedcv\", number = 10, repeats=5)\nnn.fit <- caret::train(RESPONSE ~ ., TrainData, method='nnet', trace = FALSE, \n                       ### Parameters for optmization\n                       ###learningrate = 0.0005, threshold = 0.005, stepmax = 1e+07, \n                       trControl= train_params)\n\npredict_nn <- predict(nn.fit, TestData)\n\n#library(nnet)\n#library(NeuralNetTools)\nNeuralNetTools::plotnet(nn.fit$finalModel, y_names = \"yes/no\")\ntitle(\"Graphical Representation of our Neural Network\")\n\n\n####################Confusion Matrix#################\n#bulding cm\ncm <- caret::confusionMatrix(as.factor(predict_nn), as.factor(TestData$RESPONSE))\n#################draw of confusion matrix\ndraw_confusion_matrix(cm)\n##############vv#remove temporary element\nrm(cm)\n\n\n```\n\n##### Hyper-parameter Tuning\n\n```{r neural tuning}\n\n###############################################\n\nset.seed(333) \ntrain_params_balance <- trainControl(method = \"repeatedcv\", number = 10, \n                                     repeats=5, sampling = \"down\" )\n\nnn.fit_balance <- caret::train(RESPONSE ~ ., TrainData, method='nnet', trace = FALSE, \n                       ### Parameters for optmization\n                       ###learningrate = 0.0005, threshold = 0.005, stepmax = 1e+07, \n                       trControl= train_params)\n\n```\n\n##### Predictions \n\n```{r neural predictions}\n\n#####################Predictions with the model ##########################################\n\npredict_nn_balance <- predict(nn.fit_balance, TestData)\n\n#library(nnet)\n#library(NeuralNetTools)\nNeuralNetTools::plotnet(nn.fit_balance$finalModel, y_names = \"yes/no\")\ntitle(\"Graphical Representation of our Neural Network\")\n\n\n\n```\n\n\n```{r confusion matrix neural}\n####################Confusion Matrix#################\n\n####################Confusion Matrix#################\n#bulding cm\ncm <- caret::confusionMatrix(as.factor(predict_nn_balance), as.factor(TestData$RESPONSE))\n#################draw of confusion matrix\ndraw_confusion_matrix(cm)\n##############vv#remove temporary element\nrm(cm)\n\n\n```  \n\n\n",
    "created" : 1607001092453.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2482240178",
    "id" : "AC9C99DF",
    "lastKnownWriteTime" : 1607006780,
    "last_content_update" : 1607006780362,
    "path" : "~/Desktop/-/Uni/Third Semester/Projects in Data Analytics for Decision Making/Project/Projects-DA-DecisionMaking/Project/report/model.Rmd",
    "project_path" : "report/model.Rmd",
    "properties" : {
    },
    "relative_order" : 8,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}